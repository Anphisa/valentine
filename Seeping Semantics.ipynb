{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T08:34:36.800582Z",
     "start_time": "2019-10-01T08:34:35.294050Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import jaccard_distance\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from functools import reduce\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load WE\n",
    "Download the \"iki-news-300d-1M.vec.zip\" from https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T08:37:24.908478Z",
     "start_time": "2019-10-01T08:34:38.192187Z"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movies data\n",
    "Download the movies1 data from https://sites.google.com/site/anhaidgroup/useful-stuff/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T08:37:58.595381Z",
     "start_time": "2019-10-01T08:37:58.396845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'Name', 'YearRange', 'ReleaseDate', 'Director', 'Creator', 'Cast', 'Duration', 'RatingValue', 'ContentRating', 'Genre', 'Url', 'Description']\n",
      "['Id', 'Name', 'Year', 'Release Date', 'Director', 'Creator', 'Actors', 'Cast', 'Language', 'Country', 'Duration', 'RatingValue', 'RatingCount', 'ReviewCount', 'Genre', 'Filming Locations', 'Description']\n"
     ]
    }
   ],
   "source": [
    "data_imdb = pd.read_csv('google/movies1/csv_files/imdb.csv', index_col=False)\n",
    "data_rt = pd.read_csv('google/movies1/csv_files/rotten_tomatoes.csv', index_col=False)\n",
    "data_imdb.set_index('Unnamed: 0', inplace=True)\n",
    "data_rt.set_index('Unnamed: 0', inplace=True)\n",
    "data_imdb = data_imdb.fillna('')\n",
    "data_rt = data_rt.fillna('')\n",
    "\n",
    "# get column names\n",
    "data_imdb_c = list(data_imdb.columns)\n",
    "print(data_imdb_c)\n",
    "data_rt_c = list(data_rt.columns)\n",
    "print(data_rt_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T08:38:02.979575Z",
     "start_time": "2019-10-01T08:38:02.974648Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(e):\n",
    "    capword_tokenizer = RegexpTokenizer('[A-Z]*[a-z\\d]*')\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    tokens = capword_tokenizer.tokenize(e)\n",
    "    lower_case_tokens = map(str.lower, tokens)\n",
    "    filtered_tokens = [w for w in lower_case_tokens if w and not w in stop_words] \n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherency factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T08:38:05.165117Z",
     "start_time": "2019-10-01T08:38:05.160044Z"
    }
   },
   "outputs": [],
   "source": [
    "def coherency_factor(tokens1, tokens2):\n",
    "    cf = 0\n",
    "    \n",
    "    for w1 in tokens1:\n",
    "         for w2 in tokens2:\n",
    "            if w1 != w2:\n",
    "                if w1 not in model.vocab or w2 not in model.vocab:\n",
    "                    continue\n",
    "                else:\n",
    "                    cf += model.similarity(w1, w2)\n",
    "                    \n",
    "    return cf / (len(tokens1) + len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherent group\n",
    "\n",
    "tau in [0.2, 0.4, 0.6, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T08:38:07.565010Z",
     "start_time": "2019-10-01T08:38:07.561581Z"
    }
   },
   "outputs": [],
   "source": [
    "def coherent_group(e1, e2, tau):\n",
    "    cf = coherency_factor(e1, e2)\n",
    "    return 1 if cf > tau else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance-based Matcher\n",
    "Jaccard similarity between two sets of data values. 0 represents perfect match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T08:38:09.571164Z",
     "start_time": "2019-10-01T08:38:09.565902Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(c1, c2):\n",
    "    c1_tokens = [word_tokenize(w) for w in c1]\n",
    "    c1_tokens = np.concatenate(c1_tokens).ravel()\n",
    "\n",
    "    c2_tokens = [word_tokenize(w) for w in c2]\n",
    "    c2_tokens = np.concatenate(c2_tokens).ravel()\n",
    "\n",
    "    set_c1 = set(c1_tokens)\n",
    "    set_c2 = set(c2_tokens)\n",
    "    \n",
    "#     print(len(set_c1.intersection(set_c2)))\n",
    "    \n",
    "    return jaccard_distance(set_c1, set_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T09:27:11.917769Z",
     "start_time": "2019-09-30T09:27:10.880663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.542307266895255"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = data_imdb['Creator']\n",
    "c2 = data_rt['Creator']\n",
    "\n",
    "jaccard_similarity(c1, c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T08:38:11.676320Z",
     "start_time": "2019-10-01T08:38:11.669016Z"
    }
   },
   "outputs": [],
   "source": [
    "def synm(e1, e2):\n",
    "    allsyns1 = set(ss for word in e1 for ss in wordnet.synsets(word))\n",
    "    allsyns2 = set(ss for word in e2 for ss in wordnet.synsets(word))\n",
    "    allhyps1 = set(tt for word in e1 for ss in wordnet.synsets(word) for tt in ss.hypernyms())\n",
    "    allhyps2 = set(tt for word in e2 for ss in wordnet.synsets(word) for tt in ss.hypernyms())\n",
    "    all1 = allsyns1.union(allhyps1)\n",
    "    all2 = allsyns2.union(allhyps2)\n",
    "\n",
    "    best = max((wordnet.wup_similarity(s1, s2) or 0, s1, s2) for s1, s2 in \n",
    "            product(all1, all2) if s1 != s2)\n",
    "#     print(best)\n",
    "    return best[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T08:38:15.596925Z",
     "start_time": "2019-10-01T08:38:15.389732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1 = preprocess('CellType')\n",
    "e2 = preprocess('BellType')\n",
    "\n",
    "synm(e1, e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
