{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T12:25:08.813074Z",
     "start_time": "2019-08-17T12:25:08.802594Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "sL9siW6-126f",
    "outputId": "36264746-c49d-4ee5-fd34-077118e36431",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/andra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/andra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/andra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv, string, nltk, collections, os,numbers\n",
    "import functools, operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy import stats\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from pyemd import emd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKtdmaGB2Kmk"
   },
   "source": [
    "# Phase one\n",
    "Computing distribution clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "colab_type": "text",
    "id": "R-jZOTUV2_Ms"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Google data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "dCDCCiNA2s82"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data_imdb = pd.read_csv('movies3/csv_files/imdb.csv')\n",
    "data_rt = pd.read_csv('movies3/csv_files/rotten_tomatoes.csv')\n",
    "\n",
    "# Clean data\n",
    "data_imdb = data_imdb.fillna(0)\n",
    "data_rt = data_rt.fillna(0)\n",
    "data_rt = data_rt.replace({'Rating': ['N', '.']}, {'Rating': 0})\n",
    "\n",
    "# Store data for future processing \n",
    "data1 = data_imdb\n",
    "data2 = data_rt\n",
    "\n",
    "# Get the columns\n",
    "columns1 = data1.columns\n",
    "columns2 = data2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Dummy numerical data based on the imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "vkteemSm-SgU"
   },
   "outputs": [],
   "source": [
    "dataX = pd.DataFrame(data1.Rating)\n",
    "dataX.columns = ['Score']\n",
    "dataX = dataX.reindex(np.random.permutation(dataX.index)).reset_index()\n",
    "dataX['Rating'] = data1.Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Small test data and CountVectorizer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: addresses, chicago, illinois, media, obama, president, press, speaks\n"
     ]
    }
   ],
   "source": [
    "d1 = \"Obama speaks to the media in Illinois\"\n",
    "d2 = \"The President addresses the press in Chicago\"\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\").fit([d1, d2])\n",
    "print(\"Features:\",  \", \".join(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### TPC-H Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T12:15:43.381241Z",
     "start_time": "2019-08-17T12:15:42.546122Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# customer = pd.read_csv('tpch/customer.csv', sep='|', header=None)\n",
    "# lineitem = pd.read_csv('tpch/lineitem.csv', sep='|', header=None)\n",
    "# nation = pd.read_csv('tpch/nation.csv', sep='|', header=None)\n",
    "# orders = pd.read_csv('tpch/orders.csv', sep='|', header=None)\n",
    "# part = pd.read_csv('tpch/part.csv', sep='|', header=None)\n",
    "# region = pd.read_csv('tpch/region.csv', sep='|', header=None)\n",
    "# supplier = pd.read_csv('tpch/supplier.csv', sep='|', header=None)\n",
    "# partsupp = pd.read_csv('tpch/partsupp.csv', sep='|', header=None)\n",
    "\n",
    "customer = pd.read_csv('tpch/customer.csv', sep='|')\n",
    "# lineitem = pd.read_csv('tpch/lineitem.csv', sep='|')\n",
    "# nation = pd.read_csv('tpch/nation.csv', sep='|')\n",
    "# orders = pd.read_csv('tpch/orders.csv', sep='|')\n",
    "part = pd.read_csv('tpch/part.csv', sep='|')\n",
    "# region = pd.read_csv('tpch/region.csv', sep='|')\n",
    "# supplier = pd.read_csv('tpch/supplier.csv', sep='|')\n",
    "# partsupp = pd.read_csv('tpch/partsupp.csv', sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1 from the paper \n",
    "Algorithmically identifying the cutoff EMD threshold for a column C, given a global threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T12:46:40.260508Z",
     "start_time": "2019-08-17T12:46:40.254498Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1ySKdAKV3I-1"
   },
   "outputs": [],
   "source": [
    "def compute_cutoff_threshold(C, threshold):\n",
    "    t = {}\n",
    "    t['e'] = threshold+0.001\n",
    "    t['c'] = 0\n",
    "    C.append(t)\n",
    "    C = sorted(C, key = lambda i: i['e']) \n",
    "    cutoff = 0\n",
    "    gap = 0.0\n",
    "    i = 0\n",
    "    while C[i + 1]['e'] <= threshold:\n",
    "        if gap < (C[i+1]['e'] - C[i]['e']):\n",
    "            gap = C[i+1]['e'] - C[i]['e']\n",
    "            cutoff = C[i]['e']\n",
    "        i += 1\n",
    "\n",
    "    return cutoff      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2 from the paper\n",
    "Compute distribution graph and distribution cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T12:50:25.025794Z",
     "start_time": "2019-08-17T12:50:25.017194Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "p82vPsPPIhIG"
   },
   "outputs": [],
   "source": [
    "def compute_distribution_clusters(data, columns, threshold):\n",
    "    graph = {}\n",
    "    A = {}\n",
    "#     vocab_dict = get_vocabulary()\n",
    "  \n",
    "    for i in range(0, len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "#             try:\n",
    "#                 e = wasserstein_distance(data[columns[i]], data[columns[j]])\n",
    "#             except ValueError:\n",
    "#                 e = word_emd(data[columns[i]], data[columns[j]], vocab_dict)\n",
    "            e = quantile_emd(data[columns[i]], data[columns[j]])\n",
    "            item_j = {}\n",
    "            item_j['e'] = e\n",
    "            item_j['c'] = columns[j]\n",
    "            if columns[i] not in A:\n",
    "                A[columns[i]] = []\n",
    "            A[columns[i]].append(item_j)\n",
    "\n",
    "            item_i = {}\n",
    "            item_i['e'] = e\n",
    "            item_i['c'] = columns[i]\n",
    "            if columns[j] not in A:\n",
    "                A[columns[j]] = []\n",
    "            A[columns[j]].append(item_i)\n",
    "        graph[columns[i]] = []\n",
    "    \n",
    "    for i in range(len(columns)):\n",
    "        theta = compute_cutoff_threshold(A[columns[i]], threshold)\n",
    "        Nc = get_neighbors(A[columns[i]], theta)\n",
    "        graph[columns[i]].extend(Nc)\n",
    "      \n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the neighbors of a column\n",
    "The neighborhood NC of column C consists of all columns C′ with EMD(C, C′) ≤ cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T12:15:53.091248Z",
     "start_time": "2019-08-17T12:15:53.088222Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KGgx6DGlHyvF"
   },
   "outputs": [],
   "source": [
    "def get_neighbors(C, cutoff):\n",
    "    return [i['c'] for i in C if i['e'] <= cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile EMD \n",
    "The final version of emd according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T12:47:19.564934Z",
     "start_time": "2019-08-17T12:47:19.549360Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantile_emd(column1, column2):\n",
    "    quantile = 1\n",
    "#     punct = '!\"&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'\n",
    "#     sign = '#$@%'\n",
    "    table = str.maketrans({key: ' ' for key in string.punctuation})\n",
    "#     table = str.maketrans({key: ' ' for key in punct})\n",
    "#     table.update(str.maketrans({key: '' for key in sign}))\n",
    "    \n",
    "    if type(column1) is not np.ndarray:\n",
    "        column1 = np.array(column1)\n",
    "    if type(column2) is not np.ndarray:\n",
    "        column2 = np.array(column2) \n",
    "\n",
    "    # if data is string, process it (apply lowercase, remove punctuation and tokenize the strings)\n",
    "    if (type(column1[0]) is str) or (type(column1[0]) is np.str_):\n",
    "        column1 = column1.astype(str)\n",
    "        column1 = np.chararray.translate(column1, table)\n",
    "        column1 = np.char.lower(column1)\n",
    "        column1 = [nltk.word_tokenize(token) for token in column1]\n",
    "        column1 = np.concatenate(column1).ravel()\n",
    "        \n",
    "    if (type(column2[0]) is str) or (type(column2[0]) is np.str_):  \n",
    "        column2 = column2.astype(str)\n",
    "        column2 = np.chararray.translate(column2, table)\n",
    "        column2 = np.char.lower(column2)\n",
    "        column2 = [nltk.word_tokenize(token) for token in column2]\n",
    "        column2 = np.concatenate(column2).ravel()\n",
    "        \n",
    "    # get the unique values\n",
    "    set1 = set(column1)\n",
    "    set2 = set(column2)\n",
    "    \n",
    "    # compute the union of the 2 columns\n",
    "    set_union = list(set1.union(set2))\n",
    "    \n",
    "    # sort the values in lexicographi/numeric order\n",
    "    numeric_values = np.array([x for x in set_union if isinstance(x, numbers.Number)]).astype(np.object)\n",
    "    string_values = np.array(list(filter(lambda x: x not in numeric_values, set_union)))\n",
    "    sorted_set = np.append(numeric_values, string_values, axis=0)\n",
    "\n",
    "    # rank the sorted values\n",
    "    wmap = {key: i for (i, key) in enumerate(sorted_set)}\n",
    "    ranks = np.array(list(wmap.values()))\n",
    "    ranks_l = len(ranks)\n",
    "\n",
    "    # get the ranks for each column\n",
    "    ranks1 = sorted([itemgetter(*list(set1))(wmap)])\n",
    "    l1 = len(ranks1)\n",
    "    ranks2 = sorted([itemgetter(*list(set2))(wmap)])\n",
    "    \n",
    "    # get the bin edges by using 1-quantile\n",
    "    bin_edges1 = stats.mstats.mquantiles(ranks1, np.array(range(0, l1 + 1, quantile)) / l1)\n",
    "\n",
    "    # compute the histogram for both columns\n",
    "    hist1, bins1 = np.histogram(ranks1, bins=bin_edges1)\n",
    "    hist2, bins2 = np.histogram(ranks2, bins=bin_edges1)\n",
    "    \n",
    "    # find the distance matrix between each word\n",
    "    # computation of D for 1/4 of data ~ 1 min\n",
    "    D = pdist(ranks.reshape(-1, 1), 'minkowski', p=1.)\n",
    "    \n",
    "    # computation of emd for 1/4 of data ~ 15 mins\n",
    "    e = emd(hist1 / ranks_l, hist2 / ranks_l, squareform(D)) / ranks_l\n",
    "    \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T12:45:50.863942Z",
     "start_time": "2019-08-17T12:45:50.859632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.02\n",
      "0.04\n",
      "0.06\n",
      "0.08\n",
      "0.1\n",
      "0.12000000000000001\n",
      "0.14\n"
     ]
    }
   ],
   "source": [
    "x = 0.0\n",
    "\n",
    "while x <= threshold:\n",
    "    print(x)\n",
    "    x += 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T12:51:08.581818Z",
     "start_time": "2019-08-17T12:51:08.525874Z"
    }
   },
   "outputs": [],
   "source": [
    "rows = 100\n",
    "customer = pd.read_csv('tpch/customer.csv', sep='|', nrows=rows)\n",
    "lineitem = pd.read_csv('tpch/lineitem.csv', sep='|', nrows=rows)\n",
    "nation = pd.read_csv('tpch/nation.csv', sep='|', nrows=rows)\n",
    "orders = pd.read_csv('tpch/orders.csv', sep='|', nrows=rows)\n",
    "part = pd.read_csv('tpch/part.csv', sep='|', nrows=rows)\n",
    "region = pd.read_csv('tpch/region.csv', sep='|', nrows=rows)\n",
    "supplier = pd.read_csv('tpch/supplier.csv', sep='|', nrows=rows)\n",
    "partsupp = pd.read_csv('tpch/partsupp.csv', sep='|', nrows=rows)\n",
    "\n",
    "data = pd.concat([customer, lineitem, nation, orders, part, region, supplier, partsupp], axis=1)\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset used in the paper\n",
    "Using only 1/4 for testing purposes and because my laptop runs out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T12:54:41.130611Z",
     "start_time": "2019-08-17T12:54:41.125594Z"
    }
   },
   "outputs": [],
   "source": [
    "# psc = np.array(part['P_Comment'][0:10]).astype(str)\n",
    "# psc = np.array(partsupp[4][0:10])\n",
    "psc = part['P_PartKey']\n",
    "psc = np.array(psc[0:int(len(psc) / 4)])\n",
    "# psc = np.array(psc)[0:10]\n",
    "# print(type(psc[1]))\n",
    "\n",
    "# custc = customer['C_Comment']\n",
    "# custc = customer[7][0:10]\n",
    "custc = customer['C_CustKey']\n",
    "custc = np.array(custc[0:int(len(custc) / 4)])\n",
    "# custc = np.array(custc[0:10])\n",
    "# print(type(custc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the methods given different data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XUZr_OoBSp4x",
    "outputId": "a78ce469-34ed-42e0-874d-61df1e2e9060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': [], 'Score': 'Rating', 'Rating': 'Score'}\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.14\n",
    "\n",
    "g = compute_distribution_clusters(dataX, dataX.columns, threshold)\n",
    "# g = compute_distribution_clusters(data1, data1.columns, threshold)\n",
    "\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T12:51:34.074363Z",
     "start_time": "2019-08-17T12:51:11.146245Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.14\n",
    "C = compute_distribution_clusters(data, data.columns, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T12:54:51.825513Z",
     "start_time": "2019-08-17T12:54:51.820327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_emd(psc, custc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the emd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_emd(psc, custc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Trial and error - Algorithms that don't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Get the google news vocabulary and store it as a memory map - Used for Word_EMD\n",
    "The function returns an array:\n",
    "- result[0] the data map\n",
    "- result[1] the vocabulary map\n",
    "\n",
    "Download the data from: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T18:39:52.708530Z",
     "start_time": "2019-08-16T18:39:52.699379Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_vocabulary():\n",
    "    if not os.path.exists(\"data/embed.dat\"):\n",
    "        print(\"Caching word embeddings in memmapped format...\")\n",
    "        from gensim.models import KeyedVectors\n",
    "        wv = KeyedVectors.load_word2vec_format(\n",
    "            \"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "        wv.init_sims()\n",
    "        from tempfile import mkdtemp\n",
    "        import os.path as path\n",
    "        dat_file = path.join(mkdtemp(), 'embed.dat')\n",
    "        vocab_file = path.join(mkdtemp(), 'embed.vocab')\n",
    "        fp = np.memmap(dat_file, dtype=np.double, mode='w+', shape=wv.vectors_norm.shape)\n",
    "        fp[:] = wv.vectors_norm[:]\n",
    "        with open(vocab_file, \"w+\") as f:\n",
    "            for _, w in sorted((voc.index, word) for word, voc in wv.vocab.items()):\n",
    "                print(w, file=f)\n",
    "        del fp, wv\n",
    "\n",
    "    W = np.memmap(dat_file, dtype=np.double, mode=\"r\", shape=(3000000, 300))\n",
    "    with open(vocab_file) as f:\n",
    "        vocab_list = map(str.strip, f.readlines())\n",
    "\n",
    "    return [W, {w: k for k, w in enumerate(vocab_list)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = get_vocabulary()\n",
    "W = result[0]\n",
    "vocab_dict = result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Word EMD - NOT working\n",
    "Word EMD algortihm according to: https://vene.ro/blog/word-movers-distance-in-python.html which is based on the paper: http://mkusner.github.io/publications/WMD.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def word_emd(d1, d2, vocab_dict):\n",
    "    corpus = d1 + d2\n",
    "    l1 = len(d1)\n",
    "\n",
    "    vect = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
    "    W_ = W[[vocab_dict[w] if w in vocab_dict else vocab_dict['unk'] for w in vect.get_feature_names()] ]\n",
    "    D_ = euclidean_distances(W_)\n",
    "    D_ = D_.astype(np.double)\n",
    "    D_ /= D_.max() \n",
    "    \n",
    "    v_ = vect.transform(corpus)\n",
    "    v_1 = v_[:l1,:]\n",
    "    v_2 = v_[l1:, :]\n",
    "    \n",
    "    print(v_1.shape)\n",
    "    print(v_1)\n",
    "    print(v_2.shape)\n",
    "    print(v_2)\n",
    "    \n",
    "    v_1 = v_1.toarray().ravel()\n",
    "    v_2 = v_2.toarray().ravel()\n",
    "    v_1 = v_1.astype(np.double)\n",
    "    v_2 = v_2.astype(np.double)\n",
    "    v_1 /= v_1.sum()\n",
    "    v_2 /= v_2.sum()\n",
    "    \n",
    "    print(v_1.shape)\n",
    "    print(v_2.shape)\n",
    "    print(D_.shape)\n",
    "    \n",
    "    from pyemd import emd\n",
    "\n",
    "    return [emd(v_1, v_2, D_), vect.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "(1, 8)\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "(8,)\n",
      "(8,)\n",
      "(8, 8)\n",
      "[0.744844, ['addresses', 'chicago', 'illinois', 'media', 'obama', 'president', 'press', 'speaks']]\n"
     ]
    }
   ],
   "source": [
    "# d1 = 'Massimo Morini'\n",
    "# d2 = 'Beppe Mecconi'\n",
    "\n",
    "# e = word_emd(data1['Director'].astype(str).tolist(), data1['Creators'].astype(str).tolist(), vocab_dict)\n",
    "e = word_emd([d1], [d2], vocab_dict)\n",
    "print(e)\n",
    "# data1['Creators'].tolist() + data1['Director'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['obama', 'speaks', 'to', 'the', 'media', 'in', 'illinois']\n",
      "['the', 'president', 'addresses', 'the', 'press', 'in', 'chicago']\n",
      "['addresses', 'chicago', 'illinois', 'in', 'media', 'obama', 'president', 'press', 'speaks', 'the', 'to']\n",
      "{'addresses': 0, 'chicago': 1, 'illinois': 2, 'media': 3, 'obama': 4, 'president': 5, 'press': 6, 'speaks': 7}\n"
     ]
    }
   ],
   "source": [
    "d11 = nltk.word_tokenize(d1.lower())\n",
    "d12 = nltk.word_tokenize(d2.lower())\n",
    "\n",
    "print(d11)\n",
    "print(d12)\n",
    "\n",
    "uni = set(d11).union(set(d12))\n",
    "sort = sorted(list(uni))\n",
    "print(sort)\n",
    "del sort[3]\n",
    "del sort[8]\n",
    "del sort[8]\n",
    "\n",
    "wmap = {key: i for (i, key) in enumerate(sort)}\n",
    "print(wmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Copied wasserstein distance code from python repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _cdf_distance(p, u_values, v_values, u_weights=None, v_weights=None):\n",
    "    r\"\"\"\n",
    "    Compute, between two one-dimensional distributions :math:`u` and\n",
    "    :math:`v`, whose respective CDFs are :math:`U` and :math:`V`, the\n",
    "    statistical distance that is defined as:\n",
    "    .. math::\n",
    "        l_p(u, v) = \\left( \\int_{-\\infty}^{+\\infty} |U-V|^p \\right)^{1/p}\n",
    "    p is a positive parameter; p = 1 gives the Wasserstein distance, p = 2\n",
    "    gives the energy distance.\n",
    "    Parameters\n",
    "    ----------\n",
    "    u_values, v_values : array_like\n",
    "        Values observed in the (empirical) distribution.\n",
    "    u_weights, v_weights : array_like, optional\n",
    "        Weight for each value. If unspecified, each value is assigned the same\n",
    "        weight.\n",
    "        `u_weights` (resp. `v_weights`) must have the same length as\n",
    "        `u_values` (resp. `v_values`). If the weight sum differs from 1, it\n",
    "        must still be positive and finite so that the weights can be normalized\n",
    "        to sum to 1.\n",
    "    Returns\n",
    "    -------\n",
    "    distance : float\n",
    "        The computed distance between the distributions.\n",
    "    Notes\n",
    "    -----\n",
    "    The input distributions can be empirical, therefore coming from samples\n",
    "    whose values are effectively inputs of the function, or they can be seen as\n",
    "    generalized functions, in which case they are weighted sums of Dirac delta\n",
    "    functions located at the specified values.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Bellemare, Danihelka, Dabney, Mohamed, Lakshminarayanan, Hoyer,\n",
    "           Munos \"The Cramer Distance as a Solution to Biased Wasserstein\n",
    "           Gradients\" (2017). :arXiv:`1705.10743`.\n",
    "    \"\"\"\n",
    "#     u_values, u_weights = _validate_distribution(u_values, u_weights)\n",
    "#     v_values, v_weights = _validate_distribution(v_values, v_weights)\n",
    "\n",
    "    u_sorter = np.argsort(u_values)\n",
    "    v_sorter = np.argsort(v_values)\n",
    "\n",
    "    all_values = np.concatenate((u_values, v_values))\n",
    "    all_values.sort(kind='mergesort')\n",
    "\n",
    "    # Compute the differences between pairs of successive values of u and v.\n",
    "    deltas = np.diff(all_values)\n",
    "\n",
    "    # Get the respective positions of the values of u and v among the values of\n",
    "    # both distributions.\n",
    "    u_cdf_indices = u_values[u_sorter].searchsorted(all_values[:-1], 'right')\n",
    "    v_cdf_indices = v_values[v_sorter].searchsorted(all_values[:-1], 'right')\n",
    "\n",
    "    # Calculate the CDFs of u and v using their weights, if specified.\n",
    "    if u_weights is None:\n",
    "        u_cdf = u_cdf_indices / u_values.size\n",
    "    else:\n",
    "        u_sorted_cumweights = np.concatenate(([0],\n",
    "                                              np.cumsum(u_weights[u_sorter])))\n",
    "        u_cdf = u_sorted_cumweights[u_cdf_indices] / u_sorted_cumweights[-1]\n",
    "\n",
    "    if v_weights is None:\n",
    "        v_cdf = v_cdf_indices / v_values.size\n",
    "    else:\n",
    "        v_sorted_cumweights = np.concatenate(([0],\n",
    "                                              np.cumsum(v_weights[v_sorter])))\n",
    "        v_cdf = v_sorted_cumweights[v_cdf_indices] / v_sorted_cumweights[-1]\n",
    "\n",
    "    # Compute the value of the integral based on the CDFs.\n",
    "    # If p = 1 or p = 2, we avoid using np.power, which introduces an overhead\n",
    "    # of about 15%.\n",
    "    if p == 1:\n",
    "        return np.sum(np.multiply(np.abs(u_cdf - v_cdf), deltas))\n",
    "    if p == 2:\n",
    "        return np.sqrt(np.sum(np.multiply(np.square(u_cdf - v_cdf), deltas)))\n",
    "    return np.power(np.sum(np.multiply(np.power(np.abs(u_cdf - v_cdf), p),\n",
    "                                       deltas)), 1/p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Same algorithm as above\n",
    "Tried to make it work for strings - NOT WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[1 0 2]\n",
      "[3 0 4 2 1]\n",
      "['i' 'popcorn' 'like' 'bike' 'iou']\n",
      "['bike' 'i' 'iou' 'like' 'popcorn']\n",
      "[3 2 3 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_sorter = np.argsort(u)\n",
    "print(u_sorter)\n",
    "v_sorter = np.argsort(v)\n",
    "print(v_sorter)\n",
    "\n",
    "all_values = np.concatenate((u, v))\n",
    "all_values = np.array(list(set(all_values)))\n",
    "\n",
    "all_args = np.argsort(all_values)\n",
    "print(all_args)\n",
    "print(all_values)\n",
    "all_values.sort(kind='mergesort')\n",
    "print(all_values)\n",
    "\n",
    "# uni = set(d11).union(set(d12))\n",
    "# sort = sorted(list(uni))\n",
    "# print(list(uni))\n",
    "# print(sort)\n",
    "\n",
    "# arg = np.argsort(list(uni))\n",
    "# print(arg)\n",
    "\n",
    "fv = np.vectorize(nltk.edit_distance)\n",
    "deltas = fv(all_values[:-1], all_values[1:])\n",
    "# deltas = np.diff(all_args)\n",
    "# deltas = np.diff(all_values)\n",
    "print(deltas)\n",
    "\n",
    "u_cdf_indices = np.array(u)[u_sorter].searchsorted(all_values[:-1], 'right')\n",
    "v_cdf_indices = np.array(v)[v_sorter].searchsorted(all_values[:-1], 'right')\n",
    "\n",
    "# print(all_values[:-1])\n",
    "\n",
    "# print(np.array(u)[u_sorter])\n",
    "# print(v[v_sorter])\n",
    "\n",
    "# print(u_cdf_indices)\n",
    "# print(v_cdf_indices)\n",
    "\n",
    "u_cdf = u_cdf_indices / np.array(u).size\n",
    "v_cdf = v_cdf_indices / np.array(v).size\n",
    "\n",
    "# print(u_cdf)\n",
    "# print(v_cdf)\n",
    "\n",
    "np.sum(np.multiply(np.abs(u_cdf - v_cdf), deltas))\n",
    "# np.sum(np.abs(u_cdf - v_cdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Quantile histogram algorithm - NOT working\n",
    "Copied from https://cobr.io/blog/implementing-a-multi-column-foreign-key-discovery-algorithm.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def quantilehistogram(values, numbins=256):\n",
    "        try:\n",
    "            lists = [list(t) for t in zip(*values)] # unpack pairs of values into a list of lists\n",
    "        except:\n",
    "            lists = [list(t) for t in zip(values)]\n",
    "\n",
    "        if len(lists) == 0: # empty column...\n",
    "            return None\n",
    "\n",
    "        hists = []\n",
    "        for l in lists:\n",
    "            binsize = int(math.sqrt(len(l)))\n",
    "            if binsize >= 500:\n",
    "                binsize = 499\n",
    "\n",
    "            hist = []\n",
    "            bins = []\n",
    "            try:\n",
    "                # print('trying as is..')\n",
    "                sum(l)\n",
    "                hist, bins = np.histogram(l, bins=binsize, density=True) # sqrt to improve accuracy for larger tables\n",
    "            except:\n",
    "                try:\n",
    "                    # print('trying to cast to ints..')\n",
    "                    castlist = [ int(value) for value in l ]\n",
    "                    hist, bins = np.histogram(castlist, bins=binsize, density=True)\n",
    "                except:\n",
    "                    # print('trying as is hashed..')\n",
    "                    hashedlist = [ hash(value) for value in l ]\n",
    "                    hist, bins = np.histogram(hashedlist, bins=binsize, density=True)\n",
    "                # c = collections.Counter(l)\n",
    "                # rhist = list(map((lambda x: x/len(l)), list(c.values()))) # for each quantile (map) divide by total number of records to get probability\n",
    "                # rbins = list(c)\n",
    "\n",
    "                # for i in range(numbins):\n",
    "                # \tif i < len(rhist) and i < len(rbins):\n",
    "                # \t\thist.append(rbins[i])\n",
    "                # \t\tbins.append(rhist[i])\n",
    "                # \telse:\n",
    "                # \t\thist.append(0)\n",
    "                # \t\tbins.append(0)\n",
    "                # bins.append(0)\n",
    "            hists.append((list(hist), list(bins)))\n",
    "\n",
    "        return hists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Quantile EMD - NOT working\n",
    "Source: https://cobr.io/blog/implementing-a-multi-column-foreign-key-discovery-algorithm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def q_emd(qfk, qpk):\n",
    "    emdscore = 0\n",
    "    for i in range(len(qfk)):\n",
    "        fkhist = qfk[i][0]\n",
    "        pkhist = qpk[i][0]\n",
    "\n",
    "        fkbins = qfk[i][1]\n",
    "        pkbins = qpk[i][1]\n",
    "#         print(np.transpose(np.array(fkhist)))\n",
    "        emdscore += emd(np.transpose(np.array(fkhist)), np.transpose(np.array(pkhist)), \n",
    "                        np.ascontiguousarray(np.array([fkbins[0:-1], pkbins[0:-1]]).T))\n",
    "    emdscore = emdscore/len(qfk[0])\n",
    "    return emdscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hist1 = quantilehistogram(data1['Title'][:100])\n",
    "# hist2 = quantilehistogram(data2['Title'][:100])\n",
    "\n",
    "hist1 = quantilehistogram(d11)\n",
    "hist2 = quantilehistogram(d12)\n",
    "\n",
    "# hist2 = quantilehistogram(dataX['Score'][:100])\n",
    "# q_emd(hist1, hist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "q_emd(hist1, hist2)\n",
    "# np.array([hist1[1][1][0:-1], hist2[1][1][0:-1]]).T.shape\n",
    "# np.concatenate((np.array(hist1[1][1][0:-1]), np.array(hist2[1][1][0:-1])))\n",
    "\n",
    "# dataX['Score'][:100]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Clustering.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "559px",
    "left": "1099px",
    "right": "20px",
    "top": "119px",
    "width": "321px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
