{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:45:49.221259Z",
     "start_time": "2019-08-19T19:45:47.897898Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "sL9siW6-126f",
    "outputId": "36264746-c49d-4ee5-fd34-077118e36431",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/andra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/andra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/andra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv, string, nltk, collections, os, numbers, math\n",
    "import functools, operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy import stats\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from pyemd import emd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKtdmaGB2Kmk"
   },
   "source": [
    "# Phase 1\n",
    "Computing distribution clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "colab_type": "text",
    "id": "R-jZOTUV2_Ms"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Google data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T10:09:52.514116Z",
     "start_time": "2019-08-18T10:09:52.426692Z"
    },
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "dCDCCiNA2s82"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data_imdb = pd.read_csv('movies3/csv_files/imdb.csv')\n",
    "data_rt = pd.read_csv('movies3/csv_files/rotten_tomatoes.csv')\n",
    "\n",
    "# Clean data\n",
    "data_imdb = data_imdb.fillna(0)\n",
    "data_rt = data_rt.fillna(0)\n",
    "data_rt = data_rt.replace({'Rating': ['N', '.']}, {'Rating': 0})\n",
    "\n",
    "# Store data for future processing \n",
    "data1 = data_imdb\n",
    "data2 = data_rt\n",
    "\n",
    "# Get the columns\n",
    "columns1 = data1.columns\n",
    "columns2 = data2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Dummy numerical data based on the imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T10:09:54.641153Z",
     "start_time": "2019-08-18T10:09:54.632527Z"
    },
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "vkteemSm-SgU"
   },
   "outputs": [],
   "source": [
    "dataX = pd.DataFrame(data1.Rating)\n",
    "dataX.columns = ['Score']\n",
    "dataX = dataX.reindex(np.random.permutation(dataX.index)).reset_index()\n",
    "dataX['Rating'] = data1.Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Small test data and CountVectorizer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: addresses, chicago, illinois, media, obama, president, press, speaks\n"
     ]
    }
   ],
   "source": [
    "d1 = \"Obama speaks to the media in Illinois\"\n",
    "d2 = \"The President addresses the press in Chicago\"\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\").fit([d1, d2])\n",
    "print(\"Features:\",  \", \".join(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### TPC-H Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:46:20.070077Z",
     "start_time": "2019-08-19T19:46:19.216835Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# customer = pd.read_csv('tpch/customer.csv', sep='|', header=None)\n",
    "# lineitem = pd.read_csv('tpch/lineitem.csv', sep='|', header=None)\n",
    "# nation = pd.read_csv('tpch/nation.csv', sep='|', header=None)\n",
    "# orders = pd.read_csv('tpch/orders.csv', sep='|', header=None)\n",
    "# part = pd.read_csv('tpch/part.csv', sep='|', header=None)\n",
    "# region = pd.read_csv('tpch/region.csv', sep='|', header=None)\n",
    "# supplier = pd.read_csv('tpch/supplier.csv', sep='|', header=None)\n",
    "# partsupp = pd.read_csv('tpch/partsupp.csv', sep='|', header=None)\n",
    "\n",
    "customer = pd.read_csv('tpch/customer.csv', sep='|')\n",
    "# lineitem = pd.read_csv('tpch/lineitem.csv', sep='|')\n",
    "# nation = pd.read_csv('tpch/nation.csv', sep='|')\n",
    "# orders = pd.read_csv('tpch/orders.csv', sep='|')\n",
    "part = pd.read_csv('tpch/part.csv', sep='|')\n",
    "# region = pd.read_csv('tpch/region.csv', sep='|')\n",
    "# supplier = pd.read_csv('tpch/supplier.csv', sep='|')\n",
    "# partsupp = pd.read_csv('tpch/partsupp.csv', sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1 from the paper \n",
    "Algorithmically identifying the cutoff EMD threshold for a column C, given a global threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:45:53.139978Z",
     "start_time": "2019-08-19T19:45:53.133748Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1ySKdAKV3I-1"
   },
   "outputs": [],
   "source": [
    "def compute_cutoff_threshold(C, threshold):\n",
    "    t = {}\n",
    "    t['e'] = threshold+0.001\n",
    "    t['c'] = 0\n",
    "    C.append(t)\n",
    "    C = sorted(C, key = lambda i: i['e']) \n",
    "    cutoff = 0\n",
    "    gap = 0.0\n",
    "    i = 0\n",
    "    while C[i + 1]['e'] <= threshold:\n",
    "        if gap < (C[i+1]['e'] - C[i]['e']):\n",
    "            gap = C[i+1]['e'] - C[i]['e']\n",
    "            cutoff = C[i]['e']\n",
    "        i += 1\n",
    "\n",
    "    return cutoff      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2 from the paper\n",
    "Compute distribution graph and distribution cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:45:54.603932Z",
     "start_time": "2019-08-19T19:45:54.595186Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "p82vPsPPIhIG"
   },
   "outputs": [],
   "source": [
    "def compute_distribution_clusters(data, columns, threshold):\n",
    "    graph = {}\n",
    "    A = {}\n",
    "#     vocab_dict = get_vocabulary()\n",
    "  \n",
    "    for i in tqdm(range(0, len(columns))):\n",
    "        for j in tqdm(range(i + 1, len(columns))):\n",
    "#             try:\n",
    "#                 e = wasserstein_distance(data[columns[i]], data[columns[j]])\n",
    "#             except ValueError:\n",
    "#                 e = word_emd(data[columns[i]], data[columns[j]], vocab_dict)\n",
    "            e = quantile_emd(data[columns[i]], data[columns[j]])\n",
    "            item_j = {}\n",
    "            item_j['e'] = e\n",
    "            item_j['c'] = columns[j]\n",
    "            if columns[i] not in A:\n",
    "                A[columns[i]] = []\n",
    "            A[columns[i]].append(item_j)\n",
    "\n",
    "            item_i = {}\n",
    "            item_i['e'] = e\n",
    "            item_i['c'] = columns[i]\n",
    "            if columns[j] not in A:\n",
    "                A[columns[j]] = []\n",
    "            A[columns[j]].append(item_i)\n",
    "        graph[columns[i]] = set()\n",
    "    \n",
    "    for i in range(len(columns)):\n",
    "        theta = compute_cutoff_threshold(A[columns[i]], threshold)\n",
    "        Nc = get_neighbors(A[columns[i]], theta)\n",
    "        graph[columns[i]].update(Nc)\n",
    "      \n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the neighbors of a column\n",
    "The neighborhood NC of column C consists of all columns C′ with EMD(C, C′) ≤ cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:45:57.071385Z",
     "start_time": "2019-08-19T19:45:57.067922Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KGgx6DGlHyvF"
   },
   "outputs": [],
   "source": [
    "def get_neighbors(C, cutoff):\n",
    "    return [i['c'] for i in C if i['e'] <= cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile EMD \n",
    "The final version of emd according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T20:22:27.101126Z",
     "start_time": "2019-08-19T20:22:27.082991Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantile_emd(column1, column2):\n",
    "    quantile = 256\n",
    "#     punct = '!\"&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'\n",
    "#     sign = '#$@%'\n",
    "    table = str.maketrans({key: ' ' for key in string.punctuation})\n",
    "#     table = str.maketrans({key: ' ' for key in punct})\n",
    "#     table.update(str.maketrans({key: '' for key in sign}))\n",
    "    \n",
    "    if type(column1) is not np.ndarray:\n",
    "        column1 = np.array(column1)\n",
    "    if type(column2) is not np.ndarray:\n",
    "        column2 = np.array(column2) \n",
    "\n",
    "    # if data is string, process it (apply lowercase, remove punctuation and tokenize the strings)\n",
    "    if (type(column1[0]) is str) or (type(column1[0]) is np.str_):\n",
    "        column1 = column1.astype(str)\n",
    "        column1 = np.chararray.translate(column1, table)\n",
    "        column1 = np.char.lower(column1)\n",
    "        column1 = [nltk.word_tokenize(token) for token in column1]\n",
    "        column1 = np.concatenate(column1).ravel()\n",
    "        \n",
    "    if (type(column2[0]) is str) or (type(column2[0]) is np.str_):  \n",
    "        column2 = column2.astype(str)\n",
    "        column2 = np.chararray.translate(column2, table)\n",
    "        column2 = np.char.lower(column2)\n",
    "        column2 = [nltk.word_tokenize(token) for token in column2]\n",
    "        column2 = np.concatenate(column2).ravel()\n",
    "\n",
    "    # get the unique values\n",
    "    set1 = set(column1)\n",
    "    set2 = set(column2)\n",
    "\n",
    "    # compute the union of the 2 columns\n",
    "    set_union = list(set1.union(set2))\n",
    "    \n",
    "    # sort the values in lexicographic/numeric order\n",
    "    numeric_values = np.array([x for x in set_union if isinstance(x, numbers.Number)]).astype(np.object)\n",
    "    string_values = np.array(list(filter(lambda x: x not in numeric_values, set_union)))\n",
    "    sorted_set = np.append(numeric_values, string_values, axis=0)\n",
    "\n",
    "    # rank the sorted values\n",
    "    wmap = {key: i for (i, key) in enumerate(sorted_set)}\n",
    "    ranks = np.array(list(wmap.values()))\n",
    "\n",
    "    # get the ranks for each column\n",
    "    ranks1 = np.array(itemgetter(*list(set1))(wmap))\n",
    "    ranks2 = np.array(itemgetter(*list(set2))(wmap))\n",
    "    \n",
    "    # check if the ranks contain more than 1 value and sort the lists\n",
    "    if len(ranks1.shape) > 0:\n",
    "        ranks1 = sorted(ranks1)\n",
    "        l1 = len(ranks1)\n",
    "    else:\n",
    "        l1 = 1\n",
    "\n",
    "    if len(ranks2.shape) > 0:\n",
    "        ranks2 = sorted(ranks2)\n",
    "    \n",
    "    # get the bin edges by using 1-quantile\n",
    "    bin_edges1 = stats.mstats.mquantiles(ranks1, np.array(range(0, l1 + 1, quantile)) / l1)\n",
    "\n",
    "    # compute the histogram for both columns\n",
    "    hist1, bins1 = np.histogram(ranks1, bins=bin_edges1)\n",
    "    hist2, bins2 = np.histogram(ranks2, bins=bin_edges1)\n",
    "\n",
    "    # find the distance matrix between each word\n",
    "    # computation of D for 1/4 of data ~ 1 min\n",
    "    wmap = {key: int(i/quantile) for (i, key) in enumerate(sorted_set)}\n",
    "    ranks = np.array(list(set(wmap.values())))\n",
    "    ranks_l = len(ranks)\n",
    "    D = pdist(ranks.reshape(-1, 1), 'minkowski', p=1.)\n",
    "    \n",
    "    # computation of emd for 1/4 of data ~ 15 mins\n",
    "    e = emd(hist1 / ranks_l, hist2 / ranks_l, squareform(D)) / ranks_l\n",
    "    \n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFS\n",
    "To complete algorithm 2, all the connected components have to be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:46:00.587728Z",
     "start_time": "2019-08-19T19:46:00.583735Z"
    }
   },
   "outputs": [],
   "source": [
    "def bfs(graph, start):\n",
    "    visited, queue = set(), [start]\n",
    "    while queue:\n",
    "        vertex = queue.pop(0)\n",
    "        if vertex not in visited:\n",
    "            visited.add(vertex)\n",
    "            queue.extend(graph[vertex] - visited)\n",
    "    return visited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset used in the paper according to Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T20:20:18.764688Z",
     "start_time": "2019-08-19T20:20:12.548470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['C_CustKey', 'C_Name', 'C_Address', 'C_NationKey', 'C_Phone',\n",
      "       'C_Comment', 'N_NationKey', 'N_Name', 'N_RegionKey', 'N_Comment',\n",
      "       'O_OrderKey', 'O_CustKey', 'O_OrderPriority', 'O_Comment', 'AC_CustKey',\n",
      "       'AC_Name', 'AC_Address', 'AC_NationKey', 'AC_Phone', 'AC_Comment',\n",
      "       'EC_CustKey', 'EC_Name', 'EC_Address', 'EC_NationKey', 'EC_Phone',\n",
      "       'EC_Comment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "rows = None\n",
    "customer = pd.read_csv('tpch/customer.csv', sep='|', nrows=rows)\n",
    "# lineitem = pd.read_csv('tpch/lineitem.csv', sep='|', nrows=rows)\n",
    "nation = pd.read_csv('tpch/nation.csv', sep='|', nrows=rows)\n",
    "orders = pd.read_csv('tpch/orders.csv', sep='|', nrows=rows)\n",
    "# part = pd.read_csv('tpch/part.csv', sep='|', nrows=rows)\n",
    "# region = pd.read_csv('tpch/region.csv', sep='|', nrows=rows)\n",
    "# supplier = pd.read_csv('tpch/supplier.csv', sep='|', nrows=rows)\n",
    "# partsupp = pd.read_csv('tpch/partsupp.csv', sep='|', nrows=rows)\n",
    "\n",
    "small_cust = customer[['C_CustKey', 'C_Name', 'C_Address', 'C_NationKey', 'C_Phone', 'C_Comment']].copy()\n",
    "small_order = orders[['O_OrderKey', 'O_CustKey', 'O_OrderPriority', 'O_Comment']].copy()\n",
    "\n",
    "asian_n = nation.loc[nation['N_RegionKey'] == 2]\n",
    "asian_customer = customer.loc[customer['C_NationKey'].isin(asian_n['N_NationKey'])].copy()\n",
    "asian_customer = asian_customer[['C_CustKey', 'C_Name', 'C_Address', 'C_NationKey', 'C_Phone', 'C_Comment']]\n",
    "asian_customer.rename(columns=lambda x: 'A'+x, inplace=True)\n",
    "\n",
    "eur_n = nation.loc[nation['N_RegionKey'] == 3]\n",
    "eur_customer = customer.loc[customer['C_NationKey'].isin(eur_n['N_NationKey'])].copy()\n",
    "eur_customer = eur_customer[['C_CustKey', 'C_Name', 'C_Address', 'C_NationKey', 'C_Phone', 'C_Comment']]\n",
    "eur_customer.rename(columns=lambda x: 'E'+x, inplace=True)\n",
    "\n",
    "data = pd.concat([small_cust, nation, small_order, asian_customer, eur_customer], axis=1)\n",
    "data = data.fillna(0)\n",
    "\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset used in the paper for charts\n",
    "Using only 1/4 for testing purposes and because my laptop runs out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T19:54:18.088513Z",
     "start_time": "2019-08-19T19:54:18.085320Z"
    }
   },
   "outputs": [],
   "source": [
    "# psc = np.array(part['P_Comment'][0:10]).astype(str)\n",
    "# psc = np.array(partsupp[4][0:10])\n",
    "psc = part['P_PartKey']\n",
    "# psc = np.array(psc[0:1000])\n",
    "# psc = np.array(psc)[0:10]\n",
    "# print(type(psc[1]))\n",
    "\n",
    "# custc = customer['C_Comment']\n",
    "# custc = customer[7][0:10]\n",
    "custc = customer['C_CustKey']\n",
    "# custc = np.array(custc[500:1500])\n",
    "# custc = np.array(custc[0:10])\n",
    "# print(type(custc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the methods given different data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T10:12:25.128067Z",
     "start_time": "2019-08-18T10:12:22.489677Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XUZr_OoBSp4x",
    "outputId": "a78ce469-34ed-42e0-874d-61df1e2e9060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': set(), 'Score': {'Rating'}, 'Rating': {'Score'}}\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.14\n",
    "\n",
    "g = compute_distribution_clusters(dataX, dataX.columns, threshold)\n",
    "# g = compute_distribution_clusters(data1, data1.columns, threshold)\n",
    "\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T20:26:16.861136Z",
     "start_time": "2019-08-19T20:22:32.236840Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-bfc9edeb5e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# small_data = data.loc[0:5000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(len(small_data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_distribution_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-4b892dd72800>\u001b[0m in \u001b[0;36mcompute_distribution_clusters\u001b[0;34m(data, columns, threshold)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#             except ValueError:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#                 e = word_emd(data[columns[i]], data[columns[j]], vocab_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantile_emd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mitem_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mitem_j\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'e'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-f4f966aba252>\u001b[0m in \u001b[0;36mquantile_emd\u001b[0;34m(column1, column2)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# sort the values in lexicographic/numeric order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mnumeric_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset_union\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mstring_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumeric_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_union\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0msorted_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-f4f966aba252>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# sort the values in lexicographic/numeric order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mnumeric_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset_union\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mstring_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumeric_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_union\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0msorted_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "threshold = 0.14\n",
    "# small_data = data.loc[0:5000]\n",
    "# print(len(small_data))\n",
    "C = compute_distribution_clusters(data, data.columns, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the emd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T20:17:33.844304Z",
     "start_time": "2019-08-19T20:09:30.857150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012498749999998624\n"
     ]
    }
   ],
   "source": [
    "e = quantile_emd(psc, custc)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Intersection EMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T12:00:43.061643Z",
     "start_time": "2019-08-19T12:00:43.050599Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def intersection_emd(column1, column2):\n",
    "    table = str.maketrans({key: ' ' for key in string.punctuation})\n",
    "    \n",
    "    if type(column1) is not np.ndarray:\n",
    "        column1 = np.array(column1)\n",
    "    if type(column2) is not np.ndarray:\n",
    "        column2 = np.array(column2) \n",
    "\n",
    "    # if data is string, process it (apply lowercase, remove punctuation and tokenize the strings)\n",
    "    if (type(column1[0]) is str) or (type(column1[0]) is np.str_):\n",
    "        column1 = column1.astype(str)\n",
    "        column1 = np.chararray.translate(column1, table)\n",
    "        column1 = np.char.lower(column1)\n",
    "        column1 = [nltk.word_tokenize(token) for token in column1]\n",
    "        column1 = np.concatenate(column1).ravel()\n",
    "        \n",
    "    if (type(column2[0]) is str) or (type(column2[0]) is np.str_):  \n",
    "        column2 = column2.astype(str)\n",
    "        column2 = np.chararray.translate(column2, table)\n",
    "        column2 = np.char.lower(column2)\n",
    "        column2 = [nltk.word_tokenize(token) for token in column2]\n",
    "        column2 = np.concatenate(column2).ravel()\n",
    "\n",
    "    # get the unique values\n",
    "    set1 = set(column1)\n",
    "    set2 = set(column2)\n",
    "\n",
    "    # compute the union of the 2 columns\n",
    "    set_intersection = list(set1.intersection(set2))\n",
    "    \n",
    "    if len(set_intersection) == 0:\n",
    "        return math.inf\n",
    "    \n",
    "    e1 = quantile_emd(column1, set_intersection)\n",
    "    e2 = quantile_emd(column2, set_intersection)   \n",
    "    \n",
    "    return (e1 + e2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Algorithm 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T15:01:55.730939Z",
     "start_time": "2019-08-19T15:01:55.719846Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_attributes(data, DC, theta):\n",
    "    GA = {}\n",
    "    I = {}\n",
    "    E = np.zeros((len(DC), len(DC)))\n",
    "    M = []\n",
    "    \n",
    "    for i in tqdm(range(len(DC))):\n",
    "        for j in tqdm(range(i + 1, len(DC))):\n",
    "            e = intersection_emd(data[DC[i]], data[DC[j]])\n",
    "            item_j = {}\n",
    "            item_j['e'] = e\n",
    "            item_j['c'] = DC[j]\n",
    "            \n",
    "            if DC[i] not in I:\n",
    "                I[DC[i]] = []\n",
    "            I[DC[i]].append(item_j)\n",
    "            \n",
    "            item_i = {}\n",
    "            item_i['e'] = e\n",
    "            item_i['c'] = DC[i]\n",
    "            \n",
    "            if DC[j] not in I:\n",
    "                I[DC[j]] = []\n",
    "            I[DC[j]].append(item_i) \n",
    "        print(I[DC[i]])\n",
    "        cutoff_i = compute_cutoff_threshold(I[DC[i]], theta)\n",
    "        print(cutoff_i)\n",
    "        Nc = get_neighbors(I[DC[i]], cutoff_i)\n",
    "        print(Nc)\n",
    "        for Cj in Nc:\n",
    "            E[i][DC.index(Cj)] = 1\n",
    "        GA[DC[i]] = set()\n",
    "    M = E + np.dot(E, E)\n",
    "    for i in range(len(DC)):\n",
    "        for j in range(len(DC)):\n",
    "            if M[i][j] == 0:\n",
    "                GA[DC[i]].update(['-' + DC[j]])\n",
    "            else:\n",
    "                GA[DC[i]].update([DC[j]])\n",
    "    return GA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test Alg 3 with mock data from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T15:03:28.538407Z",
     "start_time": "2019-08-19T15:01:59.948820Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▎        | 1/8 [00:00<00:02,  2.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 2/8 [00:11<00:21,  3.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 3/8 [00:25<00:33,  6.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 4/8 [00:26<00:19,  4.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▎   | 5/8 [00:26<00:10,  3.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 6/8 [00:40<00:13,  6.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 7/8 [00:51<00:08,  8.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 8/8 [01:03<00:00,  7.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█         | 1/9 [01:03<08:30, 63.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'e': inf, 'c': 'AC_Address'}, {'e': 0.9988367492917029, 'c': 'EC_Address'}, {'e': 0.9612430667960936, 'c': 'C_Comment'}, {'e': inf, 'c': 'AC_Comment'}, {'e': inf, 'c': 'EC_Comment'}, {'e': 0.9610539975771888, 'c': 'O_Comment'}, {'e': 0.9647981011189253, 'c': 'N_Name'}, {'e': 0.9400571299535293, 'c': 'N_Comment'}]\n",
      "0\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 1/7 [00:00<00:01,  3.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▊       | 2/7 [00:00<00:01,  2.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 3/7 [00:01<00:01,  2.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 4/7 [00:01<00:00,  3.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████▏  | 5/7 [00:01<00:00,  2.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 6/7 [00:02<00:00,  2.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 7/7 [00:02<00:00,  2.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 2/9 [01:06<05:17, 45.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'e': inf, 'c': 'C_Address'}, {'e': 0.9979380522890948, 'c': 'EC_Address'}, {'e': inf, 'c': 'C_Comment'}, {'e': 0.9979518694571244, 'c': 'AC_Comment'}, {'e': 0.9979380522890948, 'c': 'EC_Comment'}, {'e': inf, 'c': 'O_Comment'}, {'e': inf, 'c': 'N_Name'}, {'e': inf, 'c': 'N_Comment'}]\n",
      "0\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  1.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 2/6 [00:00<00:01,  2.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 3/6 [00:01<00:01,  2.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 4/6 [00:01<00:00,  2.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 5/6 [00:01<00:00,  2.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 3/9 [01:08<03:14, 32.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'e': 0.9988367492917029, 'c': 'C_Address'}, {'e': 0.9979380522890948, 'c': 'AC_Address'}, {'e': inf, 'c': 'C_Comment'}, {'e': 0.9979380522890948, 'c': 'AC_Comment'}, {'e': 0.9979242351210651, 'c': 'EC_Comment'}, {'e': inf, 'c': 'O_Comment'}, {'e': inf, 'c': 'N_Name'}, {'e': inf, 'c': 'N_Comment'}]\n",
      "0\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 2/5 [00:01<00:01,  1.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 3/5 [00:07<00:04,  2.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 4/5 [00:08<00:01,  1.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 4/9 [01:21<02:12, 26.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'e': 0.9612430667960936, 'c': 'C_Address'}, {'e': inf, 'c': 'AC_Address'}, {'e': inf, 'c': 'EC_Address'}, {'e': inf, 'c': 'AC_Comment'}, {'e': inf, 'c': 'EC_Comment'}, {'e': 0.18615566850499732, 'c': 'O_Comment'}, {'e': inf, 'c': 'N_Name'}, {'e': 0.45045770063806395, 'c': 'N_Comment'}]\n",
      "0.18615566850499732\n",
      "['O_Comment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  3.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 2/4 [00:00<00:00,  3.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  3.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 5/9 [01:22<01:15, 18.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'e': inf, 'c': 'C_Address'}, {'e': 0.9979518694571244, 'c': 'AC_Address'}, {'e': 0.9979380522890948, 'c': 'EC_Address'}, {'e': inf, 'c': 'C_Comment'}, {'e': 0.9979380522890948, 'c': 'EC_Comment'}, {'e': inf, 'c': 'O_Comment'}, {'e': inf, 'c': 'N_Name'}, {'e': inf, 'c': 'N_Comment'}]\n",
      "0\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 1/3 [00:00<00:00,  2.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:00<00:00,  2.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 6/9 [01:23<00:40, 13.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'e': inf, 'c': 'C_Address'}, {'e': 0.9979380522890948, 'c': 'AC_Address'}, {'e': 0.9979242351210651, 'c': 'EC_Address'}, {'e': inf, 'c': 'C_Comment'}, {'e': 0.9979380522890948, 'c': 'AC_Comment'}, {'e': inf, 'c': 'O_Comment'}, {'e': inf, 'c': 'N_Name'}, {'e': inf, 'c': 'N_Comment'}]\n",
      "0\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 1/2 [00:00<00:00,  1.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 7/9 [01:27<00:21, 10.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'e': 0.9610539975771888, 'c': 'C_Address'}, {'e': inf, 'c': 'AC_Address'}, {'e': inf, 'c': 'EC_Address'}, {'e': 0.18615566850499732, 'c': 'C_Comment'}, {'e': inf, 'c': 'AC_Comment'}, {'e': inf, 'c': 'EC_Comment'}, {'e': inf, 'c': 'N_Name'}, {'e': 0.45623127178174966, 'c': 'N_Comment'}]\n",
      "0.18615566850499732\n",
      "['C_Comment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████▉ | 8/9 [01:28<00:07,  7.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████| 9/9 [01:28<00:00,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'e': 0.9647981011189253, 'c': 'C_Address'}, {'e': inf, 'c': 'AC_Address'}, {'e': inf, 'c': 'EC_Address'}, {'e': inf, 'c': 'C_Comment'}, {'e': inf, 'c': 'AC_Comment'}, {'e': inf, 'c': 'EC_Comment'}, {'e': inf, 'c': 'O_Comment'}, {'e': 0.9557912414608114, 'c': 'N_Comment'}]\n",
      "0\n",
      "[]\n",
      "[{'e': 0.9400571299535293, 'c': 'C_Address'}, {'e': inf, 'c': 'AC_Address'}, {'e': inf, 'c': 'EC_Address'}, {'e': 0.45045770063806395, 'c': 'C_Comment'}, {'e': inf, 'c': 'AC_Comment'}, {'e': inf, 'c': 'EC_Comment'}, {'e': 0.45623127178174966, 'c': 'O_Comment'}, {'e': 0.9557912414608114, 'c': 'N_Name'}]\n",
      "0.45045770063806395\n",
      "['C_Comment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C_Address': {'-AC_Address',\n",
       "  '-AC_Comment',\n",
       "  '-C_Address',\n",
       "  '-C_Comment',\n",
       "  '-EC_Address',\n",
       "  '-EC_Comment',\n",
       "  '-N_Comment',\n",
       "  '-N_Name',\n",
       "  '-O_Comment'},\n",
       " 'AC_Address': {'-AC_Address',\n",
       "  '-AC_Comment',\n",
       "  '-C_Address',\n",
       "  '-C_Comment',\n",
       "  '-EC_Address',\n",
       "  '-EC_Comment',\n",
       "  '-N_Comment',\n",
       "  '-N_Name',\n",
       "  '-O_Comment'},\n",
       " 'EC_Address': {'-AC_Address',\n",
       "  '-AC_Comment',\n",
       "  '-C_Address',\n",
       "  '-C_Comment',\n",
       "  '-EC_Address',\n",
       "  '-EC_Comment',\n",
       "  '-N_Comment',\n",
       "  '-N_Name',\n",
       "  '-O_Comment'},\n",
       " 'C_Comment': {'-AC_Address',\n",
       "  '-AC_Comment',\n",
       "  '-C_Address',\n",
       "  '-EC_Address',\n",
       "  '-EC_Comment',\n",
       "  '-N_Comment',\n",
       "  '-N_Name',\n",
       "  'C_Comment',\n",
       "  'O_Comment'},\n",
       " 'AC_Comment': {'-AC_Address',\n",
       "  '-AC_Comment',\n",
       "  '-C_Address',\n",
       "  '-C_Comment',\n",
       "  '-EC_Address',\n",
       "  '-EC_Comment',\n",
       "  '-N_Comment',\n",
       "  '-N_Name',\n",
       "  '-O_Comment'},\n",
       " 'EC_Comment': {'-AC_Address',\n",
       "  '-AC_Comment',\n",
       "  '-C_Address',\n",
       "  '-C_Comment',\n",
       "  '-EC_Address',\n",
       "  '-EC_Comment',\n",
       "  '-N_Comment',\n",
       "  '-N_Name',\n",
       "  '-O_Comment'},\n",
       " 'O_Comment': {'-AC_Address',\n",
       "  '-AC_Comment',\n",
       "  '-C_Address',\n",
       "  '-EC_Address',\n",
       "  '-EC_Comment',\n",
       "  '-N_Comment',\n",
       "  '-N_Name',\n",
       "  'C_Comment',\n",
       "  'O_Comment'},\n",
       " 'N_Name': {'-AC_Address',\n",
       "  '-AC_Comment',\n",
       "  '-C_Address',\n",
       "  '-C_Comment',\n",
       "  '-EC_Address',\n",
       "  '-EC_Comment',\n",
       "  '-N_Comment',\n",
       "  '-N_Name',\n",
       "  '-O_Comment'},\n",
       " 'N_Comment': {'-AC_Address',\n",
       "  '-AC_Comment',\n",
       "  '-C_Address',\n",
       "  '-EC_Address',\n",
       "  '-EC_Comment',\n",
       "  '-N_Comment',\n",
       "  '-N_Name',\n",
       "  'C_Comment',\n",
       "  'O_Comment'}}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DC = ['C_Address', 'AC_Address', 'EC_Address', 'C_Comment', 'AC_Comment', 'EC_Comment', 'O_Comment', 'N_Name', 'N_Comment']\n",
    "theta = 0.5\n",
    "compute_attributes(small_data, DC, theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Trial and error - Algorithms that don't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Get the google news vocabulary and store it as a memory map - Used for Word_EMD\n",
    "The function returns an array:\n",
    "- result[0] the data map\n",
    "- result[1] the vocabulary map\n",
    "\n",
    "Download the data from: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T18:39:52.708530Z",
     "start_time": "2019-08-16T18:39:52.699379Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_vocabulary():\n",
    "    if not os.path.exists(\"data/embed.dat\"):\n",
    "        print(\"Caching word embeddings in memmapped format...\")\n",
    "        from gensim.models import KeyedVectors\n",
    "        wv = KeyedVectors.load_word2vec_format(\n",
    "            \"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "        wv.init_sims()\n",
    "        from tempfile import mkdtemp\n",
    "        import os.path as path\n",
    "        dat_file = path.join(mkdtemp(), 'embed.dat')\n",
    "        vocab_file = path.join(mkdtemp(), 'embed.vocab')\n",
    "        fp = np.memmap(dat_file, dtype=np.double, mode='w+', shape=wv.vectors_norm.shape)\n",
    "        fp[:] = wv.vectors_norm[:]\n",
    "        with open(vocab_file, \"w+\") as f:\n",
    "            for _, w in sorted((voc.index, word) for word, voc in wv.vocab.items()):\n",
    "                print(w, file=f)\n",
    "        del fp, wv\n",
    "\n",
    "    W = np.memmap(dat_file, dtype=np.double, mode=\"r\", shape=(3000000, 300))\n",
    "    with open(vocab_file) as f:\n",
    "        vocab_list = map(str.strip, f.readlines())\n",
    "\n",
    "    return [W, {w: k for k, w in enumerate(vocab_list)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = get_vocabulary()\n",
    "W = result[0]\n",
    "vocab_dict = result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Word EMD - NOT working\n",
    "Word EMD algortihm according to: https://vene.ro/blog/word-movers-distance-in-python.html which is based on the paper: http://mkusner.github.io/publications/WMD.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def word_emd(d1, d2, vocab_dict):\n",
    "    corpus = d1 + d2\n",
    "    l1 = len(d1)\n",
    "\n",
    "    vect = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
    "    W_ = W[[vocab_dict[w] if w in vocab_dict else vocab_dict['unk'] for w in vect.get_feature_names()] ]\n",
    "    D_ = euclidean_distances(W_)\n",
    "    D_ = D_.astype(np.double)\n",
    "    D_ /= D_.max() \n",
    "    \n",
    "    v_ = vect.transform(corpus)\n",
    "    v_1 = v_[:l1,:]\n",
    "    v_2 = v_[l1:, :]\n",
    "    \n",
    "    print(v_1.shape)\n",
    "    print(v_1)\n",
    "    print(v_2.shape)\n",
    "    print(v_2)\n",
    "    \n",
    "    v_1 = v_1.toarray().ravel()\n",
    "    v_2 = v_2.toarray().ravel()\n",
    "    v_1 = v_1.astype(np.double)\n",
    "    v_2 = v_2.astype(np.double)\n",
    "    v_1 /= v_1.sum()\n",
    "    v_2 /= v_2.sum()\n",
    "    \n",
    "    print(v_1.shape)\n",
    "    print(v_2.shape)\n",
    "    print(D_.shape)\n",
    "    \n",
    "    from pyemd import emd\n",
    "\n",
    "    return [emd(v_1, v_2, D_), vect.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "(1, 8)\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "(8,)\n",
      "(8,)\n",
      "(8, 8)\n",
      "[0.744844, ['addresses', 'chicago', 'illinois', 'media', 'obama', 'president', 'press', 'speaks']]\n"
     ]
    }
   ],
   "source": [
    "# d1 = 'Massimo Morini'\n",
    "# d2 = 'Beppe Mecconi'\n",
    "\n",
    "# e = word_emd(data1['Director'].astype(str).tolist(), data1['Creators'].astype(str).tolist(), vocab_dict)\n",
    "e = word_emd([d1], [d2], vocab_dict)\n",
    "print(e)\n",
    "# data1['Creators'].tolist() + data1['Director'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['obama', 'speaks', 'to', 'the', 'media', 'in', 'illinois']\n",
      "['the', 'president', 'addresses', 'the', 'press', 'in', 'chicago']\n",
      "['addresses', 'chicago', 'illinois', 'in', 'media', 'obama', 'president', 'press', 'speaks', 'the', 'to']\n",
      "{'addresses': 0, 'chicago': 1, 'illinois': 2, 'media': 3, 'obama': 4, 'president': 5, 'press': 6, 'speaks': 7}\n"
     ]
    }
   ],
   "source": [
    "d11 = nltk.word_tokenize(d1.lower())\n",
    "d12 = nltk.word_tokenize(d2.lower())\n",
    "\n",
    "print(d11)\n",
    "print(d12)\n",
    "\n",
    "uni = set(d11).union(set(d12))\n",
    "sort = sorted(list(uni))\n",
    "print(sort)\n",
    "del sort[3]\n",
    "del sort[8]\n",
    "del sort[8]\n",
    "\n",
    "wmap = {key: i for (i, key) in enumerate(sort)}\n",
    "print(wmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Copied wasserstein distance code from python repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _cdf_distance(p, u_values, v_values, u_weights=None, v_weights=None):\n",
    "    r\"\"\"\n",
    "    Compute, between two one-dimensional distributions :math:`u` and\n",
    "    :math:`v`, whose respective CDFs are :math:`U` and :math:`V`, the\n",
    "    statistical distance that is defined as:\n",
    "    .. math::\n",
    "        l_p(u, v) = \\left( \\int_{-\\infty}^{+\\infty} |U-V|^p \\right)^{1/p}\n",
    "    p is a positive parameter; p = 1 gives the Wasserstein distance, p = 2\n",
    "    gives the energy distance.\n",
    "    Parameters\n",
    "    ----------\n",
    "    u_values, v_values : array_like\n",
    "        Values observed in the (empirical) distribution.\n",
    "    u_weights, v_weights : array_like, optional\n",
    "        Weight for each value. If unspecified, each value is assigned the same\n",
    "        weight.\n",
    "        `u_weights` (resp. `v_weights`) must have the same length as\n",
    "        `u_values` (resp. `v_values`). If the weight sum differs from 1, it\n",
    "        must still be positive and finite so that the weights can be normalized\n",
    "        to sum to 1.\n",
    "    Returns\n",
    "    -------\n",
    "    distance : float\n",
    "        The computed distance between the distributions.\n",
    "    Notes\n",
    "    -----\n",
    "    The input distributions can be empirical, therefore coming from samples\n",
    "    whose values are effectively inputs of the function, or they can be seen as\n",
    "    generalized functions, in which case they are weighted sums of Dirac delta\n",
    "    functions located at the specified values.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Bellemare, Danihelka, Dabney, Mohamed, Lakshminarayanan, Hoyer,\n",
    "           Munos \"The Cramer Distance as a Solution to Biased Wasserstein\n",
    "           Gradients\" (2017). :arXiv:`1705.10743`.\n",
    "    \"\"\"\n",
    "#     u_values, u_weights = _validate_distribution(u_values, u_weights)\n",
    "#     v_values, v_weights = _validate_distribution(v_values, v_weights)\n",
    "\n",
    "    u_sorter = np.argsort(u_values)\n",
    "    v_sorter = np.argsort(v_values)\n",
    "\n",
    "    all_values = np.concatenate((u_values, v_values))\n",
    "    all_values.sort(kind='mergesort')\n",
    "\n",
    "    # Compute the differences between pairs of successive values of u and v.\n",
    "    deltas = np.diff(all_values)\n",
    "\n",
    "    # Get the respective positions of the values of u and v among the values of\n",
    "    # both distributions.\n",
    "    u_cdf_indices = u_values[u_sorter].searchsorted(all_values[:-1], 'right')\n",
    "    v_cdf_indices = v_values[v_sorter].searchsorted(all_values[:-1], 'right')\n",
    "\n",
    "    # Calculate the CDFs of u and v using their weights, if specified.\n",
    "    if u_weights is None:\n",
    "        u_cdf = u_cdf_indices / u_values.size\n",
    "    else:\n",
    "        u_sorted_cumweights = np.concatenate(([0],\n",
    "                                              np.cumsum(u_weights[u_sorter])))\n",
    "        u_cdf = u_sorted_cumweights[u_cdf_indices] / u_sorted_cumweights[-1]\n",
    "\n",
    "    if v_weights is None:\n",
    "        v_cdf = v_cdf_indices / v_values.size\n",
    "    else:\n",
    "        v_sorted_cumweights = np.concatenate(([0],\n",
    "                                              np.cumsum(v_weights[v_sorter])))\n",
    "        v_cdf = v_sorted_cumweights[v_cdf_indices] / v_sorted_cumweights[-1]\n",
    "\n",
    "    # Compute the value of the integral based on the CDFs.\n",
    "    # If p = 1 or p = 2, we avoid using np.power, which introduces an overhead\n",
    "    # of about 15%.\n",
    "    if p == 1:\n",
    "        return np.sum(np.multiply(np.abs(u_cdf - v_cdf), deltas))\n",
    "    if p == 2:\n",
    "        return np.sqrt(np.sum(np.multiply(np.square(u_cdf - v_cdf), deltas)))\n",
    "    return np.power(np.sum(np.multiply(np.power(np.abs(u_cdf - v_cdf), p),\n",
    "                                       deltas)), 1/p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Same algorithm as above\n",
    "Tried to make it work for strings - NOT WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[1 0 2]\n",
      "[3 0 4 2 1]\n",
      "['i' 'popcorn' 'like' 'bike' 'iou']\n",
      "['bike' 'i' 'iou' 'like' 'popcorn']\n",
      "[3 2 3 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_sorter = np.argsort(u)\n",
    "print(u_sorter)\n",
    "v_sorter = np.argsort(v)\n",
    "print(v_sorter)\n",
    "\n",
    "all_values = np.concatenate((u, v))\n",
    "all_values = np.array(list(set(all_values)))\n",
    "\n",
    "all_args = np.argsort(all_values)\n",
    "print(all_args)\n",
    "print(all_values)\n",
    "all_values.sort(kind='mergesort')\n",
    "print(all_values)\n",
    "\n",
    "# uni = set(d11).union(set(d12))\n",
    "# sort = sorted(list(uni))\n",
    "# print(list(uni))\n",
    "# print(sort)\n",
    "\n",
    "# arg = np.argsort(list(uni))\n",
    "# print(arg)\n",
    "\n",
    "fv = np.vectorize(nltk.edit_distance)\n",
    "deltas = fv(all_values[:-1], all_values[1:])\n",
    "# deltas = np.diff(all_args)\n",
    "# deltas = np.diff(all_values)\n",
    "print(deltas)\n",
    "\n",
    "u_cdf_indices = np.array(u)[u_sorter].searchsorted(all_values[:-1], 'right')\n",
    "v_cdf_indices = np.array(v)[v_sorter].searchsorted(all_values[:-1], 'right')\n",
    "\n",
    "# print(all_values[:-1])\n",
    "\n",
    "# print(np.array(u)[u_sorter])\n",
    "# print(v[v_sorter])\n",
    "\n",
    "# print(u_cdf_indices)\n",
    "# print(v_cdf_indices)\n",
    "\n",
    "u_cdf = u_cdf_indices / np.array(u).size\n",
    "v_cdf = v_cdf_indices / np.array(v).size\n",
    "\n",
    "# print(u_cdf)\n",
    "# print(v_cdf)\n",
    "\n",
    "np.sum(np.multiply(np.abs(u_cdf - v_cdf), deltas))\n",
    "# np.sum(np.abs(u_cdf - v_cdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Quantile histogram algorithm - NOT working\n",
    "Copied from https://cobr.io/blog/implementing-a-multi-column-foreign-key-discovery-algorithm.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def quantilehistogram(values, numbins=256):\n",
    "        try:\n",
    "            lists = [list(t) for t in zip(*values)] # unpack pairs of values into a list of lists\n",
    "        except:\n",
    "            lists = [list(t) for t in zip(values)]\n",
    "\n",
    "        if len(lists) == 0: # empty column...\n",
    "            return None\n",
    "\n",
    "        hists = []\n",
    "        for l in lists:\n",
    "            binsize = int(math.sqrt(len(l)))\n",
    "            if binsize >= 500:\n",
    "                binsize = 499\n",
    "\n",
    "            hist = []\n",
    "            bins = []\n",
    "            try:\n",
    "                # print('trying as is..')\n",
    "                sum(l)\n",
    "                hist, bins = np.histogram(l, bins=binsize, density=True) # sqrt to improve accuracy for larger tables\n",
    "            except:\n",
    "                try:\n",
    "                    # print('trying to cast to ints..')\n",
    "                    castlist = [ int(value) for value in l ]\n",
    "                    hist, bins = np.histogram(castlist, bins=binsize, density=True)\n",
    "                except:\n",
    "                    # print('trying as is hashed..')\n",
    "                    hashedlist = [ hash(value) for value in l ]\n",
    "                    hist, bins = np.histogram(hashedlist, bins=binsize, density=True)\n",
    "                # c = collections.Counter(l)\n",
    "                # rhist = list(map((lambda x: x/len(l)), list(c.values()))) # for each quantile (map) divide by total number of records to get probability\n",
    "                # rbins = list(c)\n",
    "\n",
    "                # for i in range(numbins):\n",
    "                # \tif i < len(rhist) and i < len(rbins):\n",
    "                # \t\thist.append(rbins[i])\n",
    "                # \t\tbins.append(rhist[i])\n",
    "                # \telse:\n",
    "                # \t\thist.append(0)\n",
    "                # \t\tbins.append(0)\n",
    "                # bins.append(0)\n",
    "            hists.append((list(hist), list(bins)))\n",
    "\n",
    "        return hists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Quantile EMD - NOT working\n",
    "Source: https://cobr.io/blog/implementing-a-multi-column-foreign-key-discovery-algorithm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def q_emd(qfk, qpk):\n",
    "    emdscore = 0\n",
    "    for i in range(len(qfk)):\n",
    "        fkhist = qfk[i][0]\n",
    "        pkhist = qpk[i][0]\n",
    "\n",
    "        fkbins = qfk[i][1]\n",
    "        pkbins = qpk[i][1]\n",
    "#         print(np.transpose(np.array(fkhist)))\n",
    "        emdscore += emd(np.transpose(np.array(fkhist)), np.transpose(np.array(pkhist)), \n",
    "                        np.ascontiguousarray(np.array([fkbins[0:-1], pkbins[0:-1]]).T))\n",
    "    emdscore = emdscore/len(qfk[0])\n",
    "    return emdscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hist1 = quantilehistogram(data1['Title'][:100])\n",
    "# hist2 = quantilehistogram(data2['Title'][:100])\n",
    "\n",
    "hist1 = quantilehistogram(d11)\n",
    "hist2 = quantilehistogram(d12)\n",
    "\n",
    "# hist2 = quantilehistogram(dataX['Score'][:100])\n",
    "# q_emd(hist1, hist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "q_emd(hist1, hist2)\n",
    "# np.array([hist1[1][1][0:-1], hist2[1][1][0:-1]]).T.shape\n",
    "# np.concatenate((np.array(hist1[1][1][0:-1]), np.array(hist2[1][1][0:-1])))\n",
    "\n",
    "# dataX['Score'][:100]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Clustering.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "559px",
    "left": "1099px",
    "right": "20px",
    "top": "119px",
    "width": "321px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
