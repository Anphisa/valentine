{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T13:11:49.308812Z",
     "start_time": "2019-08-16T13:11:49.138001Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "sL9siW6-126f",
    "outputId": "36264746-c49d-4ee5-fd34-077118e36431",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/andra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/andra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/andra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv, string, nltk, collections, os\n",
    "import functools, operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy import stats\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from pyemd import emd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKtdmaGB2Kmk"
   },
   "source": [
    "# Phase one\n",
    "Computing distribution clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "colab_type": "text",
    "id": "R-jZOTUV2_Ms"
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "dCDCCiNA2s82"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data_imdb = pd.read_csv('movies3/csv_files/imdb.csv')\n",
    "data_rt = pd.read_csv('movies3/csv_files/rotten_tomatoes.csv')\n",
    "\n",
    "# Clean data\n",
    "data_imdb = data_imdb.fillna(0)\n",
    "data_rt = data_rt.fillna(0)\n",
    "data_rt = data_rt.replace({'Rating': ['N', '.']}, {'Rating': 0})\n",
    "\n",
    "# Store data for future processing \n",
    "data1 = data_imdb\n",
    "data2 = data_rt\n",
    "\n",
    "# Get the columns\n",
    "columns1 = data1.columns\n",
    "columns2 = data2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Dummy numerical data based on the imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "vkteemSm-SgU"
   },
   "outputs": [],
   "source": [
    "dataX = pd.DataFrame(data1.Rating)\n",
    "dataX.columns = ['Score']\n",
    "dataX = dataX.reindex(np.random.permutation(dataX.index)).reset_index()\n",
    "dataX['Rating'] = data1.Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Small test data and CountVectorizer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: addresses, chicago, illinois, media, obama, president, press, speaks\n"
     ]
    }
   ],
   "source": [
    "d1 = \"Obama speaks to the media in Illinois\"\n",
    "d2 = \"The President addresses the press in Chicago\"\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\").fit([d1, d2])\n",
    "print(\"Features:\",  \", \".join(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### TPC-H Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T10:46:35.079527Z",
     "start_time": "2019-08-16T10:46:34.255014Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# customer = pd.read_csv('tpch/customer.csv', sep='|', header=None)\n",
    "# lineitem = pd.read_csv('tpch/lineitem.csv', sep='|', header=None)\n",
    "# nation = pd.read_csv('tpch/nation.csv', sep='|', header=None)\n",
    "# orders = pd.read_csv('tpch/orders.csv', sep='|', header=None)\n",
    "# part = pd.read_csv('tpch/part.csv', sep='|', header=None)\n",
    "# region = pd.read_csv('tpch/region.csv', sep='|', header=None)\n",
    "# supplier = pd.read_csv('tpch/supplier.csv', sep='|', header=None)\n",
    "# partsupp = pd.read_csv('tpch/partsupp.csv', sep='|', header=None)\n",
    "\n",
    "customer = pd.read_csv('tpch/customer.csv', sep='|')\n",
    "# lineitem = pd.read_csv('tpch/lineitem.csv', sep='|')\n",
    "# nation = pd.read_csv('tpch/nation.csv', sep='|')\n",
    "# orders = pd.read_csv('tpch/orders.csv', sep='|')\n",
    "part = pd.read_csv('tpch/part.csv', sep='|')\n",
    "# region = pd.read_csv('tpch/region.csv', sep='|')\n",
    "# supplier = pd.read_csv('tpch/supplier.csv', sep='|')\n",
    "# partsupp = pd.read_csv('tpch/partsupp.csv', sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Get the google news vocabulary and store it as a memory map - Used for Word_EMD\n",
    "The function returns an array:\n",
    "- result[0] the data map\n",
    "- result[1] the vocabulary map\n",
    "\n",
    "Download the data from: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "hidden": true,
    "id": "ujzciel9-yxL",
    "outputId": "1e582dd7-848a-406f-e0da-7f8164311e40"
   },
   "outputs": [],
   "source": [
    "def get_vocabulary():\n",
    "    if not os.path.exists(\"data/embed.dat\"):\n",
    "        print(\"Caching word embeddings in memmapped format...\")\n",
    "        from gensim.models import KeyedVectors\n",
    "        wv = KeyedVectors.load_word2vec_format(\n",
    "            \"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "        wv.init_sims()\n",
    "        from tempfile import mkdtemp\n",
    "        import os.path as path\n",
    "        dat_file = path.join(mkdtemp(), 'embed.dat')\n",
    "        vocab_file = path.join(mkdtemp(), 'embed.vocab')\n",
    "        fp = np.memmap(dat_file, dtype=np.double, mode='w+', shape=wv.vectors_norm.shape)\n",
    "        fp[:] = wv.vectors_norm[:]\n",
    "        with open(vocab_file, \"w+\") as f:\n",
    "            for _, w in sorted((voc.index, word) for word, voc in wv.vocab.items()):\n",
    "                print(w, file=f)\n",
    "        del fp, wv\n",
    "\n",
    "    W = np.memmap(dat_file, dtype=np.double, mode=\"r\", shape=(3000000, 300))\n",
    "    with open(vocab_file) as f:\n",
    "        vocab_list = map(str.strip, f.readlines())\n",
    "\n",
    "    return [W, {w: k for k, w in enumerate(vocab_list)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching word embeddings in memmapped format...\n"
     ]
    }
   ],
   "source": [
    "result = get_vocabulary()\n",
    "W = result[0]\n",
    "vocab_dict = result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1 from the paper \n",
    "Algorithmically identifying the cutoff EMD threshold for a column C, given a global threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T13:11:55.563726Z",
     "start_time": "2019-08-16T13:11:55.559453Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1ySKdAKV3I-1"
   },
   "outputs": [],
   "source": [
    "def compute_cutoff_threshold(C, threshold):\n",
    "    t = {}\n",
    "    t['e'] = threshold\n",
    "    t['c'] = 0\n",
    "    C.append(t)\n",
    "    C = sorted(C, key = lambda i: i['e']) \n",
    "    cutoff = 0\n",
    "    gap = 0.0\n",
    "    i = 0\n",
    "    while C[i + 1]['e'] <= threshold:\n",
    "        if gap < (C[i+1]['e'] - C[i]['e']):\n",
    "            gap = C[i+1]['e'] - C[i]['e']\n",
    "            cutoff = C[i]['e']\n",
    "        i += 1\n",
    "\n",
    "    return cutoff      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the neighbors of a column\n",
    "The neighborhood NC of column C consists of all columns C′ with EMD(C, C′) ≤ cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T13:11:56.694926Z",
     "start_time": "2019-08-16T13:11:56.691725Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KGgx6DGlHyvF"
   },
   "outputs": [],
   "source": [
    "def get_neighbors(C, cutoff):\n",
    "    return [i['c'] for i in C if i['e'] <= cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2 from the paper\n",
    "Compute distribution graph and distribution cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T13:11:58.733126Z",
     "start_time": "2019-08-16T13:11:58.723645Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "p82vPsPPIhIG"
   },
   "outputs": [],
   "source": [
    "def compute_distribution_clusters(data, columns, threshold):\n",
    "    graph = {}\n",
    "    A = {}\n",
    "#     vocab_dict = get_vocabulary()\n",
    "  \n",
    "    for i in range(0, len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "#             try:\n",
    "#                 e = wasserstein_distance(data[columns[i]], data[columns[j]])\n",
    "#             except ValueError:\n",
    "#                 e = word_emd(data[columns[i]], data[columns[j]], vocab_dict)\n",
    "            e = quantile_emd(data[columns[i]], data[columns[j]])\n",
    "            item_j = {}\n",
    "            item_j['e'] = e\n",
    "            item_j['c'] = columns[j]\n",
    "            if columns[i] not in A:\n",
    "                A[columns[i]] = []\n",
    "            A[columns[i]].append(item_j)\n",
    "\n",
    "            item_i = {}\n",
    "            item_i['e'] = e\n",
    "            item_i['c'] = columns[i]\n",
    "            if columns[j] not in A:\n",
    "                A[columns[j]] = []\n",
    "            A[columns[j]].append(item_i)\n",
    "        graph[columns[i]] = []\n",
    "    \n",
    "    for i in range(len(columns)):\n",
    "        theta = compute_cutoff_threshold(A[columns[i]], threshold)\n",
    "        Nc = get_neighbors(A[columns[i]], theta)\n",
    "\n",
    "        for c in Nc:\n",
    "            graph[columns[i]] = c\n",
    "      \n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T13:12:00.618849Z",
     "start_time": "2019-08-16T13:12:00.564063Z"
    }
   },
   "outputs": [],
   "source": [
    "customer = pd.read_csv('tpch/customer.csv', sep='|', nrows=20)\n",
    "lineitem = pd.read_csv('tpch/lineitem.csv', sep='|', nrows=20)\n",
    "nation = pd.read_csv('tpch/nation.csv', sep='|', nrows=20)\n",
    "orders = pd.read_csv('tpch/orders.csv', sep='|', nrows=20)\n",
    "part = pd.read_csv('tpch/part.csv', sep='|', nrows=20)\n",
    "region = pd.read_csv('tpch/region.csv', sep='|', nrows=20)\n",
    "supplier = pd.read_csv('tpch/supplier.csv', sep='|', nrows=20)\n",
    "partsupp = pd.read_csv('tpch/partsupp.csv', sep='|', nrows=20)\n",
    "\n",
    "data = pd.concat([customer, lineitem, nation, orders, part, region, supplier, partsupp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T15:12:36.831861Z",
     "start_time": "2019-08-16T15:12:36.679925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8324.07, 6.34, 3857.34, 5266.3, 794.47, 6819.74, 2866.83, 7603.4, 2753.54, 3396.49, 711.56, 4681.03, 7498.12, 8914.71, 7638.57, 9561.95, 2788.52, -272.6, 5494.43, 121.65}\n",
      "['000000015', 8324.07, '000000016', 6.34, 'customer', '000000018', '000000019', 3857.34, 5266.3, '000000013', 794.47, '000000010', 6819.74, '000000004', '000000002', '000000005', 2866.83, 7603.4, '000000011', '000000012', '000000007', '000000009', 2753.54, 3396.49, 711.56, '000000020', '000000017', 4681.03, 7498.12, 8914.71, 7638.57, 9561.95, 2788.52, '000000001', '000000003', '000000006', -272.6, '000000014', 5494.43, '000000008', 121.65]\n",
      "[1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0\n",
      " 17.0 18.0 19.0 20.0 '-272.6' '121.65' '2753.54' '2788.52' '2866.83'\n",
      " '3396.49' '3857.34' '4681.03' '5266.3' '5494.43' '6.34' '6819.74'\n",
      " '711.56' '7498.12' '7603.4' '7638.57' '794.47' '8324.07' '8914.71'\n",
      " '9561.95' 'customer']\n",
      "{'000000015', '000000016', 'customer', '000000018', '000000019', '000000013', '000000010', '000000004', '000000002', '000000005', '000000011', '000000012', '000000007', '000000009', '000000020', '000000017', '000000001', '000000003', '000000006', '000000014', '000000008'}\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'ranks2' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-358-f99dbd3f2d1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_distribution_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-177-98dcb154ead1>\u001b[0m in \u001b[0;36mcompute_distribution_clusters\u001b[0;34m(data, columns, threshold)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#             except ValueError:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#                 e = word_emd(data[columns[i]], data[columns[j]], vocab_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantile_emd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mitem_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mitem_j\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'e'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-357-98c2094a5e19>\u001b[0m in \u001b[0;36mquantile_emd\u001b[0;34m(column1, column2)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# compute the histogram for both columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mhist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranks1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbin_edges1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mhist2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranks2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbin_edges1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# find the distance matrix between each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'ranks2' referenced before assignment"
     ]
    }
   ],
   "source": [
    "threshold = 0.14\n",
    "C = compute_distribution_clusters(data, data.columns, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Word EMD - NOT working\n",
    "Word EMD algortihm according to: https://vene.ro/blog/word-movers-distance-in-python.html which is based on the paper: http://mkusner.github.io/publications/WMD.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def word_emd(d1, d2, vocab_dict):\n",
    "    corpus = d1 + d2\n",
    "    l1 = len(d1)\n",
    "\n",
    "    vect = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
    "    W_ = W[[vocab_dict[w] if w in vocab_dict else vocab_dict['unk'] for w in vect.get_feature_names()] ]\n",
    "    D_ = euclidean_distances(W_)\n",
    "    D_ = D_.astype(np.double)\n",
    "    D_ /= D_.max() \n",
    "    \n",
    "    v_ = vect.transform(corpus)\n",
    "    v_1 = v_[:l1,:]\n",
    "    v_2 = v_[l1:, :]\n",
    "    \n",
    "    print(v_1.shape)\n",
    "    print(v_1)\n",
    "    print(v_2.shape)\n",
    "    print(v_2)\n",
    "    \n",
    "    v_1 = v_1.toarray().ravel()\n",
    "    v_2 = v_2.toarray().ravel()\n",
    "    v_1 = v_1.astype(np.double)\n",
    "    v_2 = v_2.astype(np.double)\n",
    "    v_1 /= v_1.sum()\n",
    "    v_2 /= v_2.sum()\n",
    "    \n",
    "    print(v_1.shape)\n",
    "    print(v_2.shape)\n",
    "    print(D_.shape)\n",
    "    \n",
    "    from pyemd import emd\n",
    "\n",
    "    return [emd(v_1, v_2, D_), vect.get_feature_names()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Test the methods given different data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "XUZr_OoBSp4x",
    "outputId": "a78ce469-34ed-42e0-874d-61df1e2e9060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': [], 'Score': 'Rating', 'Rating': 'Score'}\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.14\n",
    "\n",
    "g = compute_distribution_clusters(dataX, dataX.columns, threshold)\n",
    "# g = compute_distribution_clusters(data1, data1.columns, threshold)\n",
    "\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "(1, 8)\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "(8,)\n",
      "(8,)\n",
      "(8, 8)\n",
      "[0.744844, ['addresses', 'chicago', 'illinois', 'media', 'obama', 'president', 'press', 'speaks']]\n"
     ]
    }
   ],
   "source": [
    "# d1 = 'Massimo Morini'\n",
    "# d2 = 'Beppe Mecconi'\n",
    "\n",
    "# e = word_emd(data1['Director'].astype(str).tolist(), data1['Creators'].astype(str).tolist(), vocab_dict)\n",
    "e = word_emd([d1], [d2], vocab_dict)\n",
    "print(e)\n",
    "# data1['Creators'].tolist() + data1['Director'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['obama', 'speaks', 'to', 'the', 'media', 'in', 'illinois']\n",
      "['the', 'president', 'addresses', 'the', 'press', 'in', 'chicago']\n",
      "['addresses', 'chicago', 'illinois', 'in', 'media', 'obama', 'president', 'press', 'speaks', 'the', 'to']\n",
      "{'addresses': 0, 'chicago': 1, 'illinois': 2, 'media': 3, 'obama': 4, 'president': 5, 'press': 6, 'speaks': 7}\n"
     ]
    }
   ],
   "source": [
    "d11 = nltk.word_tokenize(d1.lower())\n",
    "d12 = nltk.word_tokenize(d2.lower())\n",
    "\n",
    "print(d11)\n",
    "print(d12)\n",
    "\n",
    "uni = set(d11).union(set(d12))\n",
    "sort = sorted(list(uni))\n",
    "print(sort)\n",
    "del sort[3]\n",
    "del sort[8]\n",
    "del sort[8]\n",
    "\n",
    "wmap = {key: i for (i, key) in enumerate(sort)}\n",
    "print(wmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Copied wasserstein distance code from python repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _cdf_distance(p, u_values, v_values, u_weights=None, v_weights=None):\n",
    "    r\"\"\"\n",
    "    Compute, between two one-dimensional distributions :math:`u` and\n",
    "    :math:`v`, whose respective CDFs are :math:`U` and :math:`V`, the\n",
    "    statistical distance that is defined as:\n",
    "    .. math::\n",
    "        l_p(u, v) = \\left( \\int_{-\\infty}^{+\\infty} |U-V|^p \\right)^{1/p}\n",
    "    p is a positive parameter; p = 1 gives the Wasserstein distance, p = 2\n",
    "    gives the energy distance.\n",
    "    Parameters\n",
    "    ----------\n",
    "    u_values, v_values : array_like\n",
    "        Values observed in the (empirical) distribution.\n",
    "    u_weights, v_weights : array_like, optional\n",
    "        Weight for each value. If unspecified, each value is assigned the same\n",
    "        weight.\n",
    "        `u_weights` (resp. `v_weights`) must have the same length as\n",
    "        `u_values` (resp. `v_values`). If the weight sum differs from 1, it\n",
    "        must still be positive and finite so that the weights can be normalized\n",
    "        to sum to 1.\n",
    "    Returns\n",
    "    -------\n",
    "    distance : float\n",
    "        The computed distance between the distributions.\n",
    "    Notes\n",
    "    -----\n",
    "    The input distributions can be empirical, therefore coming from samples\n",
    "    whose values are effectively inputs of the function, or they can be seen as\n",
    "    generalized functions, in which case they are weighted sums of Dirac delta\n",
    "    functions located at the specified values.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Bellemare, Danihelka, Dabney, Mohamed, Lakshminarayanan, Hoyer,\n",
    "           Munos \"The Cramer Distance as a Solution to Biased Wasserstein\n",
    "           Gradients\" (2017). :arXiv:`1705.10743`.\n",
    "    \"\"\"\n",
    "#     u_values, u_weights = _validate_distribution(u_values, u_weights)\n",
    "#     v_values, v_weights = _validate_distribution(v_values, v_weights)\n",
    "\n",
    "    u_sorter = np.argsort(u_values)\n",
    "    v_sorter = np.argsort(v_values)\n",
    "\n",
    "    all_values = np.concatenate((u_values, v_values))\n",
    "    all_values.sort(kind='mergesort')\n",
    "\n",
    "    # Compute the differences between pairs of successive values of u and v.\n",
    "    deltas = np.diff(all_values)\n",
    "\n",
    "    # Get the respective positions of the values of u and v among the values of\n",
    "    # both distributions.\n",
    "    u_cdf_indices = u_values[u_sorter].searchsorted(all_values[:-1], 'right')\n",
    "    v_cdf_indices = v_values[v_sorter].searchsorted(all_values[:-1], 'right')\n",
    "\n",
    "    # Calculate the CDFs of u and v using their weights, if specified.\n",
    "    if u_weights is None:\n",
    "        u_cdf = u_cdf_indices / u_values.size\n",
    "    else:\n",
    "        u_sorted_cumweights = np.concatenate(([0],\n",
    "                                              np.cumsum(u_weights[u_sorter])))\n",
    "        u_cdf = u_sorted_cumweights[u_cdf_indices] / u_sorted_cumweights[-1]\n",
    "\n",
    "    if v_weights is None:\n",
    "        v_cdf = v_cdf_indices / v_values.size\n",
    "    else:\n",
    "        v_sorted_cumweights = np.concatenate(([0],\n",
    "                                              np.cumsum(v_weights[v_sorter])))\n",
    "        v_cdf = v_sorted_cumweights[v_cdf_indices] / v_sorted_cumweights[-1]\n",
    "\n",
    "    # Compute the value of the integral based on the CDFs.\n",
    "    # If p = 1 or p = 2, we avoid using np.power, which introduces an overhead\n",
    "    # of about 15%.\n",
    "    if p == 1:\n",
    "        return np.sum(np.multiply(np.abs(u_cdf - v_cdf), deltas))\n",
    "    if p == 2:\n",
    "        return np.sqrt(np.sum(np.multiply(np.square(u_cdf - v_cdf), deltas)))\n",
    "    return np.power(np.sum(np.multiply(np.power(np.abs(u_cdf - v_cdf), p),\n",
    "                                       deltas)), 1/p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Same algorithm as above\n",
    "Tried to make it work for strings - NOT WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[1 0 2]\n",
      "[3 0 4 2 1]\n",
      "['i' 'popcorn' 'like' 'bike' 'iou']\n",
      "['bike' 'i' 'iou' 'like' 'popcorn']\n",
      "[3 2 3 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_sorter = np.argsort(u)\n",
    "print(u_sorter)\n",
    "v_sorter = np.argsort(v)\n",
    "print(v_sorter)\n",
    "\n",
    "all_values = np.concatenate((u, v))\n",
    "all_values = np.array(list(set(all_values)))\n",
    "\n",
    "all_args = np.argsort(all_values)\n",
    "print(all_args)\n",
    "print(all_values)\n",
    "all_values.sort(kind='mergesort')\n",
    "print(all_values)\n",
    "\n",
    "# uni = set(d11).union(set(d12))\n",
    "# sort = sorted(list(uni))\n",
    "# print(list(uni))\n",
    "# print(sort)\n",
    "\n",
    "# arg = np.argsort(list(uni))\n",
    "# print(arg)\n",
    "\n",
    "fv = np.vectorize(nltk.edit_distance)\n",
    "deltas = fv(all_values[:-1], all_values[1:])\n",
    "# deltas = np.diff(all_args)\n",
    "# deltas = np.diff(all_values)\n",
    "print(deltas)\n",
    "\n",
    "u_cdf_indices = np.array(u)[u_sorter].searchsorted(all_values[:-1], 'right')\n",
    "v_cdf_indices = np.array(v)[v_sorter].searchsorted(all_values[:-1], 'right')\n",
    "\n",
    "# print(all_values[:-1])\n",
    "\n",
    "# print(np.array(u)[u_sorter])\n",
    "# print(v[v_sorter])\n",
    "\n",
    "# print(u_cdf_indices)\n",
    "# print(v_cdf_indices)\n",
    "\n",
    "u_cdf = u_cdf_indices / np.array(u).size\n",
    "v_cdf = v_cdf_indices / np.array(v).size\n",
    "\n",
    "# print(u_cdf)\n",
    "# print(v_cdf)\n",
    "\n",
    "np.sum(np.multiply(np.abs(u_cdf - v_cdf), deltas))\n",
    "# np.sum(np.abs(u_cdf - v_cdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Quantile histogram algorithm - NOT working\n",
    "Copied from https://cobr.io/blog/implementing-a-multi-column-foreign-key-discovery-algorithm.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def quantilehistogram(values, numbins=256):\n",
    "        try:\n",
    "            lists = [list(t) for t in zip(*values)] # unpack pairs of values into a list of lists\n",
    "        except:\n",
    "            lists = [list(t) for t in zip(values)]\n",
    "\n",
    "        if len(lists) == 0: # empty column...\n",
    "            return None\n",
    "\n",
    "        hists = []\n",
    "        for l in lists:\n",
    "            binsize = int(math.sqrt(len(l)))\n",
    "            if binsize >= 500:\n",
    "                binsize = 499\n",
    "\n",
    "            hist = []\n",
    "            bins = []\n",
    "            try:\n",
    "                # print('trying as is..')\n",
    "                sum(l)\n",
    "                hist, bins = np.histogram(l, bins=binsize, density=True) # sqrt to improve accuracy for larger tables\n",
    "            except:\n",
    "                try:\n",
    "                    # print('trying to cast to ints..')\n",
    "                    castlist = [ int(value) for value in l ]\n",
    "                    hist, bins = np.histogram(castlist, bins=binsize, density=True)\n",
    "                except:\n",
    "                    # print('trying as is hashed..')\n",
    "                    hashedlist = [ hash(value) for value in l ]\n",
    "                    hist, bins = np.histogram(hashedlist, bins=binsize, density=True)\n",
    "                # c = collections.Counter(l)\n",
    "                # rhist = list(map((lambda x: x/len(l)), list(c.values()))) # for each quantile (map) divide by total number of records to get probability\n",
    "                # rbins = list(c)\n",
    "\n",
    "                # for i in range(numbins):\n",
    "                # \tif i < len(rhist) and i < len(rbins):\n",
    "                # \t\thist.append(rbins[i])\n",
    "                # \t\tbins.append(rhist[i])\n",
    "                # \telse:\n",
    "                # \t\thist.append(0)\n",
    "                # \t\tbins.append(0)\n",
    "                # bins.append(0)\n",
    "            hists.append((list(hist), list(bins)))\n",
    "\n",
    "        return hists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Quantile EMD - NOT working\n",
    "Source: https://cobr.io/blog/implementing-a-multi-column-foreign-key-discovery-algorithm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def q_emd(qfk, qpk):\n",
    "    emdscore = 0\n",
    "    for i in range(len(qfk)):\n",
    "        fkhist = qfk[i][0]\n",
    "        pkhist = qpk[i][0]\n",
    "\n",
    "        fkbins = qfk[i][1]\n",
    "        pkbins = qpk[i][1]\n",
    "#         print(np.transpose(np.array(fkhist)))\n",
    "        emdscore += emd(np.transpose(np.array(fkhist)), np.transpose(np.array(pkhist)), \n",
    "                        np.ascontiguousarray(np.array([fkbins[0:-1], pkbins[0:-1]]).T))\n",
    "    emdscore = emdscore/len(qfk[0])\n",
    "    return emdscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hist1 = quantilehistogram(data1['Title'][:100])\n",
    "# hist2 = quantilehistogram(data2['Title'][:100])\n",
    "\n",
    "hist1 = quantilehistogram(d11)\n",
    "hist2 = quantilehistogram(d12)\n",
    "\n",
    "# hist2 = quantilehistogram(dataX['Score'][:100])\n",
    "# q_emd(hist1, hist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "q_emd(hist1, hist2)\n",
    "# np.array([hist1[1][1][0:-1], hist2[1][1][0:-1]]).T.shape\n",
    "# np.concatenate((np.array(hist1[1][1][0:-1]), np.array(hist2[1][1][0:-1])))\n",
    "\n",
    "# dataX['Score'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile EMD \n",
    "The final version of emd according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T15:12:33.861452Z",
     "start_time": "2019-08-16T15:12:33.843480Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantile_emd(column1, column2):\n",
    "    quantile = 1\n",
    "#     punct = '!\"&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'\n",
    "#     sign = '#$@%'\n",
    "    table = str.maketrans({key: ' ' for key in string.punctuation})\n",
    "#     table = str.maketrans({key: ' ' for key in punct})\n",
    "#     table.update(str.maketrans({key: '' for key in sign}))\n",
    "    \n",
    "    if type(column1) is not np.ndarray:\n",
    "        column1 = np.array(column1)\n",
    "    if type(column2) is not np.ndarray:\n",
    "        column2 = np.array(column2) \n",
    "\n",
    "    # if data is string, process it (apply lowercase, remove punctuation and tokenize the strings)\n",
    "    if (type(column1[0]) is str) or (type(column1[0]) is np.str_):\n",
    "        column1 = column1.astype(str)\n",
    "        column1 = np.chararray.translate(column1, table)\n",
    "        column1 = np.char.lower(column1)\n",
    "        column1 = [nltk.word_tokenize(token) for token in column1]\n",
    "        column1 = np.concatenate(column1).ravel()\n",
    "        \n",
    "    if (type(column2[0]) is str) or (type(column2[0]) is np.str_):   \n",
    "        column2 = column2.astype(str)\n",
    "        column2 = np.chararray.translate(column2, table)\n",
    "        column2 = np.char.lower(column2)\n",
    "        column2 = [nltk.word_tokenize(token) for token in column2]\n",
    "        column2 = np.concatenate(column2).ravel()\n",
    "        \n",
    "    # get the unique values\n",
    "    set1 = set(column1)\n",
    "    set2 = set(column2)\n",
    "    \n",
    "    # compute the union of the 2 columns\n",
    "    set_union = list(set1.union(set2))\n",
    "    \n",
    "    # sort the values in lexicographi/numeric order\n",
    "    try:\n",
    "        condition = np.char.isnumeric(set_union)\n",
    "        numeric_values = np.sort(list(set(np.extract(condition, set_union).astype(float)))).astype(np.object)\n",
    "        string_values = np.sort(np.extract(~condition, set_union)).astype(np.object)\n",
    "        sorted_set = np.append(numeric_values, string_values, axis=0)\n",
    "    except:\n",
    "        sorted_set = sorted(set_union)\n",
    "\n",
    "    # rank the words in lexicographic/numeric order\n",
    "    wmap = {key: i for (i, key) in enumerate(sorted_set)}\n",
    "    ranks = np.array(list(wmap.values()))\n",
    "    ranks_l = len(ranks)\n",
    "\n",
    "    # get the ranks for all the data for each column\n",
    "    ranks1 = sorted([(itemgetter(*[float(text) if str(text).isdigit() else text for text in list(set1)])(wmap))])\n",
    "    l1 = len(ranks1)\n",
    "\n",
    "    try:\n",
    "        ranks2 = sorted([(itemgetter(*[float(text) if str(text).isdigit() else text for text in list(set2)])(wmap))])\n",
    "    except:\n",
    "        print(set2)\n",
    "        print(set_union)\n",
    "        print(sorted_set)\n",
    "        print(set1)\n",
    "    # get the bin edges by using 1-quantile\n",
    "    bin_edges1 = stats.mstats.mquantiles(ranks1, np.array(range(0, l1 + 1, quantile)) / l1)\n",
    "\n",
    "    # compute the histogram for both columns\n",
    "    hist1, bins1 = np.histogram(ranks1, bins=bin_edges1)\n",
    "    hist2, bins2 = np.histogram(ranks2, bins=bin_edges1)\n",
    "    \n",
    "    # find the distance matrix between each word\n",
    "    # computation of D for 1/4 of data ~ 1 min\n",
    "    D = pdist(ranks.reshape(-1, 1), 'minkowski', p=1.)\n",
    "    \n",
    "    # computation of emd for 1/4 of data ~ 15 mins\n",
    "    e = emd(hist1 / ranks_l, hist2 / ranks_l, squareform(D)) / ranks_l\n",
    "\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T15:09:10.290246Z",
     "start_time": "2019-08-16T15:09:10.279674Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "isnumeric is only available for Unicode strings and arrays",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-348-ecc927761b4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m123.78\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnumeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Envs/thesis/lib/python3.7/site-packages/numpy/core/defchararray.py\u001b[0m in \u001b[0;36misnumeric\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   1786\u001b[0m     \"\"\"\n\u001b[1;32m   1787\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0municode_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1788\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"isnumeric is only available for Unicode strings and arrays\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1789\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_vec_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'isnumeric'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: isnumeric is only available for Unicode strings and arrays"
     ]
    }
   ],
   "source": [
    "a = [123.78]\n",
    "\n",
    "np.char.isnumeric(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset used in the paper\n",
    "Using only 1/4 for testing purposes and because my laptop runs out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psc = np.array(part['P_Comment'][0:10]).astype(str)\n",
    "# psc = np.array(partsupp[4][0:10])\n",
    "psc = part['P_PartKey']\n",
    "psc = np.array(psc[0:int(len(psc) / 4)])\n",
    "# psc = np.array(psc)[0:10]\n",
    "# print(type(psc[1]))\n",
    "\n",
    "# custc = customer['C_Comment']\n",
    "# custc = customer[7][0:10]\n",
    "custc = customer['C_CustKey']\n",
    "custc = np.array(custc[0:int(len(custc) / 4)])\n",
    "# custc = np.array(custc[0:10])\n",
    "# print(type(custc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the emd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T12:41:58.627079Z",
     "start_time": "2019-08-16T12:39:52.541633Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-3e40bc81c806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquantile_emd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-120-1186bcdc7dd5>\u001b[0m in \u001b[0;36mquantile_emd\u001b[0;34m(column1, column2)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'minkowski'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mranks_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mranks_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquareform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mranks_l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/thesis/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36msquareform\u001b[0;34m(X, force, checks)\u001b[0m\n\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m         \u001b[0;31m# Fill in the values of the distance matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2185\u001b[0;31m         \u001b[0m_distance_wrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_squareform_from_vector_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2187\u001b[0m         \u001b[0;31m# Return the distance matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "quantile_emd(psc, custc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Clustering.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "559px",
    "left": "1099px",
    "right": "20px",
    "top": "119px",
    "width": "321px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
