{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:24.578868Z",
     "start_time": "2019-11-11T08:30:23.666794Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import snakecase\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from similarity.ngram import NGram\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:25.995624Z",
     "start_time": "2019-11-11T08:30:25.993524Z"
    }
   },
   "outputs": [],
   "source": [
    "data = 'POList. Order #. 01-Id.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:26.007993Z",
     "start_time": "2019-11-11T08:30:26.006244Z"
    }
   },
   "outputs": [],
   "source": [
    "data = 'Abbreviations and acronyms are expanded, e.g. {PO, Lines} 01 {Purchase, Order, Lines}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:26.811956Z",
     "start_time": "2019-11-11T08:30:26.807796Z"
    }
   },
   "outputs": [],
   "source": [
    "table = { 'name': 'nation',\n",
    "         'columns': [\n",
    "             { 'name': 'n_nationkey',\n",
    "             'type': 'INTEGER'\n",
    "             }, \n",
    "             { 'name': 'n_name',\n",
    "             'type': 'CHAR(25)'\n",
    "             },\n",
    "             { 'name': 'n_regionkey',\n",
    "             'type': 'INTEGER'\n",
    "             },\n",
    "             { 'name': 'n_comment',\n",
    "             'type': 'VARCHAR(152)'\n",
    "             }\n",
    "         ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:29.627905Z",
     "start_time": "2019-11-11T08:30:29.615704Z"
    }
   },
   "outputs": [],
   "source": [
    "class SchemaElement:\n",
    "    def __init__(self):\n",
    "        self.category = list() # an element can belong to multiple categories\n",
    "        self.tokens = list()\n",
    "        self.table_name = None\n",
    "        \n",
    "    def add_category(self, category):\n",
    "        self.category.append(category)\n",
    "        \n",
    "    def add_token(self, token):\n",
    "        if type(token) is Token:\n",
    "            self.tokens.append(token)\n",
    "        else:\n",
    "            print(\"Incorrect token type. The type should be 'Token'\")\n",
    "            \n",
    "    def get_tokens_data(self, tokens=None):\n",
    "        if tokens is None:\n",
    "            return list(map(lambda t: t.data, self.tokens)) \n",
    "        else:\n",
    "            return list(map(lambda t: t.data, tokens)) \n",
    "    \n",
    "    def get_tokens_data_type(self, tokens=None):\n",
    "        if tokens is None:\n",
    "            return list(map(lambda t: (t.data, t.token_type), self.tokens))\n",
    "        else:\n",
    "            return list(map(lambda t: (t.data, t.token_type), tokens))\n",
    "    \n",
    "    def sort_by_token_type(self):\n",
    "        return sorted(self.tokens, key=lambda token: token.token_type.token_name)\n",
    "    \n",
    "    def get_tokens_by_token_type(self, token_type):\n",
    "        sorted_tokens = self.sort_by_token_type()\n",
    "        return list(filter(lambda t: t.token_type == token_type, sorted_tokens))\n",
    "        \n",
    "class Token:\n",
    "    def __init__(self):\n",
    "        self.ignore = False\n",
    "        self.data = None\n",
    "        self.token_type = None\n",
    "        \n",
    "TokenType = namedtuple('TokenType', ['token_name', 'weight'])\n",
    "        \n",
    "class TokenTypes(Enum):\n",
    "    SYMBOLS = TokenType('symbols', 0)\n",
    "    NUMBER = TokenType('number', 0.1)\n",
    "    COMMON_WORDS = TokenType('common words', 0.1)\n",
    "    CONTENT = TokenType('content', 0.8)\n",
    "        \n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.value.weight\n",
    "    \n",
    "    @property\n",
    "    def token_name(self):\n",
    "        return self.value.token_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:30.582137Z",
     "start_time": "2019-11-11T08:30:30.577929Z"
    }
   },
   "outputs": [],
   "source": [
    "class Table:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.attributes = list()\n",
    "        \n",
    "    def add_attribute(self, name, attribute_type):\n",
    "        schema_element = normalize(name)\n",
    "        schema_element.category = attribute_type\n",
    "        self.attributes.append(schema_element)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:32.182295Z",
     "start_time": "2019-11-11T08:30:32.174034Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(element, schema_element=None):\n",
    "    if schema_element is None:\n",
    "        schema_element = SchemaElement()\n",
    "    tokens = nltk.word_tokenize(element)\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_obj = Token()\n",
    "        if token in string.punctuation:\n",
    "            token_obj.ignore = True\n",
    "            token_obj.data = token\n",
    "            token_obj.token_type = TokenTypes.SYMBOLS\n",
    "            schema_element.add_token(token_obj)\n",
    "        else:\n",
    "            try:\n",
    "                token_float = float(token)\n",
    "                token_obj.data = token\n",
    "                token_obj.token_type = TokenTypes.NUMBER\n",
    "                schema_element.add_token(token_obj)\n",
    "            except ValueError:\n",
    "                token_snake = snakecase.convert(token)\n",
    "                if '_' in token_snake:\n",
    "                    token_snake = token_snake.replace('_', ' ')\n",
    "                    schema_element = normalize(token_snake, schema_element)\n",
    "                elif token.lower() in stopwords.words('english'):\n",
    "                    token_obj.data = token.lower()\n",
    "                    token_obj.ignore = True\n",
    "                    token_obj.token_type = TokenTypes.COMMON_WORDS\n",
    "                    schema_element.add_token(token_obj)\n",
    "                else:\n",
    "                    token_obj.data = token.lower()\n",
    "                    token_obj.token_type = TokenTypes.CONTENT\n",
    "                    schema_element.add_token(token_obj)\n",
    "    \n",
    "    return schema_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:33.166135Z",
     "start_time": "2019-11-11T08:30:33.142082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and',\n",
       "  <TokenTypes.COMMON_WORDS: TokenType(token_name='common words', weight=0.1)>),\n",
       " ('are',\n",
       "  <TokenTypes.COMMON_WORDS: TokenType(token_name='common words', weight=0.1)>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = normalize(data)\n",
    "# s.get_tokens_data()\n",
    "# s.get_tokens_data_type()\n",
    "sbc = s.sort_by_token_type()\n",
    "# s.get_tokens_data_category()\n",
    "s.get_tokens_data_type(s.get_tokens_by_token_type(TokenTypes.COMMON_WORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:34.973229Z",
     "start_time": "2019-11-11T08:30:34.969400Z"
    }
   },
   "outputs": [],
   "source": [
    "def name_similarity_tokens(token_set1, token_set2):\n",
    "    sum1 = get_partial_similarity(token_set1, token_set2)\n",
    "    sum2 = get_partial_similarity(token_set2, token_set1)\n",
    "    \n",
    "    return (sum1 + sum2) / (len(token_set1) + len(token_set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:35.584896Z",
     "start_time": "2019-11-11T08:30:35.579536Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_partial_similarity(token_set1, token_set2):\n",
    "    total_sum = 0\n",
    "    for t1 in token_set1:\n",
    "        max_sim = -math.inf\n",
    "        for t2 in token_set2:\n",
    "            sim = compute_similarity_wordnet(t1.data, t2.data)\n",
    "            if math.isnan(sim):\n",
    "                sim = 1 - compute_similarity_ngram(t1.data, t2.data, 2)\n",
    "                \n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                \n",
    "        total_sum = total_sum + max_sim\n",
    "    \n",
    "    return total_sum            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:36.116503Z",
     "start_time": "2019-11-11T08:30:36.110789Z"
    }
   },
   "outputs": [],
   "source": [
    "# the higher, the better\n",
    "def compute_similarity_wordnet(word1, word2):\n",
    "    allsyns1 = set(ss for ss in wn.synsets(word1))\n",
    "    if len(allsyns1) == 0:\n",
    "        return math.nan\n",
    "    allsyns2 = set(ss for ss in wn.synsets(word2))\n",
    "    \n",
    "    if len(allsyns2) == 0:\n",
    "        return math.nan\n",
    "    \n",
    "    best = max((wn.wup_similarity(s1, s2) or 0, s1, s2) for s1, s2 in product(allsyns1, allsyns2))\n",
    "#     print(best)\n",
    "    \n",
    "    return best[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:36.609023Z",
     "start_time": "2019-11-11T08:30:36.605181Z"
    }
   },
   "outputs": [],
   "source": [
    "# the lower, the better\n",
    "def compute_similarity_ngram(word1, word2, N):\n",
    "    ngram = NGram(N)\n",
    "    sim = ngram.distance(word1, word2)\n",
    "#     print(sim)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:37.544634Z",
     "start_time": "2019-11-11T08:30:37.487614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6266666666666667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = \"PODeliverTo\"\n",
    "d2 = \"ShipTO\"\n",
    "\n",
    "s1 = normalize(d1)\n",
    "# s1.get_tokens_data_type()\n",
    "\n",
    "s2 = normalize(d2)\n",
    "# s2.get_tokens_data_type()\n",
    "\n",
    "name_similarity_tokens(s1.tokens, s2.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:39.044051Z",
     "start_time": "2019-11-11T08:30:39.037800Z"
    }
   },
   "outputs": [],
   "source": [
    "# max is 0.5\n",
    "def name_similarity_elements(element1, element2):\n",
    "    sum1 = 0 \n",
    "    sum2 = 0\n",
    "    \n",
    "    for tt in TokenTypes:\n",
    "        if tt == TokenTypes.SYMBOLS:\n",
    "            continue \n",
    "        t1 = element1.get_tokens_by_token_type(tt)\n",
    "        t2 = element2.get_tokens_by_token_type(tt)\n",
    "        \n",
    "        if len(t1) == 0 or len(t2) == 0:\n",
    "            continue\n",
    "            \n",
    "        sim = name_similarity_tokens(t1, t2)\n",
    "        sum1 = sum1 + tt.weight * sim\n",
    "        sum2 = sum2 + tt.weight * (len(t1) + len(t2))\n",
    "        \n",
    "    return sum1/sum2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:40.005637Z",
     "start_time": "2019-11-11T08:30:39.986726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15470085470085468"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_similarity_elements(s1, s2)\n",
    "# s1.get_tokens_data_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:43.282070Z",
     "start_time": "2019-11-11T08:30:43.276896Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_lsim(element1, element2): \n",
    "    ns = name_similarity_elements(element1, element2)\n",
    "    print(ns)\n",
    "    max_c = -math.inf\n",
    "    for c1 in element1.category:\n",
    "        c1 = normalize(c1)\n",
    "        for c2 in element2.category:\n",
    "            c2 = normalize(c2)\n",
    "            nsc = name_similarity_elements(c1, c2)\n",
    "            if nsc > max_c:\n",
    "                max_c = nsc\n",
    "    print(max_c)\n",
    "    \n",
    "    return ns * max_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T08:30:44.540244Z",
     "start_time": "2019-11-11T08:30:44.452855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ship', <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>), ('to', <TokenTypes.COMMON_WORDS: TokenType(token_name='common words', weight=0.1)>)]\n",
      "0.5\n",
      "0.5\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "d1 = \"ShipTo\"\n",
    "d2 = \"ShipTO\"\n",
    "\n",
    "s1 = normalize(d1)\n",
    "s2 = normalize(d2)\n",
    "\n",
    "s1.add_category('CHAR(25)')\n",
    "s2.add_category('CHAR(25)')\n",
    "\n",
    "print(s1.get_tokens_data_type())\n",
    "\n",
    "lsim = compute_lsim(s1, s2)\n",
    "print(lsim)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
