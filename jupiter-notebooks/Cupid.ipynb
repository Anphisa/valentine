{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T08:44:15.979949Z",
     "start_time": "2019-11-14T08:44:15.022859Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import snakecase\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from similarity.ngram import NGram\n",
    "from itertools import product\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "from anytree import Node, RenderTree, PostOrderIter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T18:39:07.607603Z",
     "start_time": "2019-11-12T18:39:07.605851Z"
    }
   },
   "outputs": [],
   "source": [
    "data = 'POList. Order #. 01-Id.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T19:50:22.606488Z",
     "start_time": "2019-11-13T19:50:22.603791Z"
    }
   },
   "outputs": [],
   "source": [
    "data = 'Abbreviations and acronyms are expanded, e.g. {PO, Lines} 01 {Purchase, Order, Lines}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T18:39:07.641864Z",
     "start_time": "2019-11-12T18:39:07.639236Z"
    }
   },
   "outputs": [],
   "source": [
    "table = { 'name': 'nation',\n",
    "         'columns': [\n",
    "             { 'name': 'n_nationkey',\n",
    "             'type': 'INTEGER'\n",
    "             }, \n",
    "             { 'name': 'n_name',\n",
    "             'type': 'CHAR(25)'\n",
    "             },\n",
    "             { 'name': 'n_regionkey',\n",
    "             'type': 'INTEGER'\n",
    "             },\n",
    "             { 'name': 'n_comment',\n",
    "             'type': 'VARCHAR(152)'\n",
    "             }\n",
    "         ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T08:48:56.418174Z",
     "start_time": "2019-11-14T08:48:56.404149Z"
    }
   },
   "outputs": [],
   "source": [
    "class SchemaElement:\n",
    "    def __init__(self, name):\n",
    "        self.categories = list()\n",
    "        self.data_type = None # an element can belong to multiple categories\n",
    "        self.tokens = list()\n",
    "        self.initial_name = name\n",
    "#         self.table_name = None\n",
    "        \n",
    "    def add_category(self, category):\n",
    "        self.categories.append(category)\n",
    "        \n",
    "    def add_token(self, token):\n",
    "        if type(token) is Token:\n",
    "            self.tokens.append(token)\n",
    "        else:\n",
    "            print(\"Incorrect token type. The type should be 'Token'\")\n",
    "            \n",
    "    def get_tokens_data(self, tokens=None):\n",
    "        if tokens is None:\n",
    "            return list(map(lambda t: t.data, self.tokens)) \n",
    "        else:\n",
    "            return list(map(lambda t: t.data, tokens)) \n",
    "    \n",
    "    def get_tokens_data_type(self, tokens=None):\n",
    "        if tokens is None:\n",
    "            return list(map(lambda t: (t.data, t.token_type), self.tokens))\n",
    "        else:\n",
    "            return list(map(lambda t: (t.data, t.token_type), tokens))\n",
    "    \n",
    "    def sort_by_token_type(self):\n",
    "        return sorted(self.tokens, key=lambda token: token.token_type.token_name)\n",
    "    \n",
    "    def get_tokens_by_token_type(self, token_type):\n",
    "        sorted_tokens = self.sort_by_token_type()\n",
    "        return list(filter(lambda t: t.token_type == token_type, sorted_tokens))\n",
    "        \n",
    "class Token:\n",
    "    def __init__(self):\n",
    "        self.ignore = False\n",
    "        self.data = None\n",
    "        self.token_type = None\n",
    "        \n",
    "TokenType = namedtuple('TokenType', ['token_name', 'weight'])\n",
    "        \n",
    "class TokenTypes(Enum):\n",
    "    SYMBOLS = TokenType('symbols', 0)\n",
    "    NUMBER = TokenType('number', 0.1)\n",
    "    COMMON_WORDS = TokenType('common words', 0.1)\n",
    "    CONTENT = TokenType('content', 0.8)\n",
    "        \n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.value.weight\n",
    "    \n",
    "    @property\n",
    "    def token_name(self):\n",
    "        return self.value.token_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T08:44:29.871950Z",
     "start_time": "2019-11-14T08:44:29.865440Z"
    }
   },
   "outputs": [],
   "source": [
    "class Table:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.columns = list()\n",
    "        \n",
    "    def add_column(self, column_name, column_type=None):\n",
    "        schema_element = normalize(column_name)\n",
    "        if column_type:\n",
    "            schema_element.category = column_type\n",
    "        self.columns.append(schema_element)\n",
    "        \n",
    "    def get_all_columns(self):\n",
    "        return list(map(lambda c: c.get_tokens_data_type(), self.columns))\n",
    "    \n",
    "    def get_column_by_type(self, column_type):\n",
    "        return list(map(lambda c: c.get_tokens_data_type(),\n",
    "            filter(lambda c: column_type in c.category, self.columns)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T08:44:31.684800Z",
     "start_time": "2019-11-14T08:44:31.679334Z"
    }
   },
   "outputs": [],
   "source": [
    "class Schema:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.tables = list()\n",
    "        \n",
    "    def add_table_by_name(self, table_name):\n",
    "        table = Table(table_name)\n",
    "        self.tables.append(table)\n",
    "        \n",
    "    def add_table(self, table):\n",
    "        self.tables.append(table)\n",
    "                \n",
    "    def get_all_tables(self):\n",
    "        return list(map(lambda t: t.get_all_columns(), self.tables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:17:44.468259Z",
     "start_time": "2019-11-14T14:17:44.459890Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(element, schema_element=None):\n",
    "    if schema_element is None:\n",
    "        schema_element = SchemaElement(element)\n",
    "    tokens = nltk.word_tokenize(element)\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_obj = Token()\n",
    "        if token in string.punctuation:\n",
    "            token_obj.ignore = True\n",
    "            token_obj.data = token\n",
    "            token_obj.token_type = TokenTypes.SYMBOLS\n",
    "#             token_obj.initial_name = token\n",
    "            schema_element.add_token(token_obj)\n",
    "        else:\n",
    "            try:\n",
    "                token_float = float(token)\n",
    "                token_obj.data = token\n",
    "                token_obj.token_type = TokenTypes.NUMBER\n",
    "#                 token_obj.initial_name = token\n",
    "                schema_element.add_token(token_obj)\n",
    "            except ValueError:\n",
    "                token_snake = snakecase.convert(token)\n",
    "                if '_' in token_snake:\n",
    "                    token_snake = token_snake.replace('_', ' ')\n",
    "                    schema_element = normalize(token_snake, schema_element)\n",
    "                elif token.lower() in stopwords.words('english'):\n",
    "                    token_obj.data = token.lower()\n",
    "                    token_obj.ignore = True\n",
    "                    token_obj.token_type = TokenTypes.COMMON_WORDS\n",
    "#                     token_obj.initial_name = token\n",
    "                    schema_element.add_token(token_obj)\n",
    "                else:\n",
    "                    token_obj.data = token.lower()\n",
    "                    token_obj.token_type = TokenTypes.CONTENT\n",
    "#                     token_obj.initial_name = token\n",
    "                    schema_element.add_token(token_obj)\n",
    "    \n",
    "    return schema_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T19:50:35.141875Z",
     "start_time": "2019-11-13T19:50:35.133116Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abbreviations',\n",
       "  <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('acronyms',\n",
       "  <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('expanded',\n",
       "  <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('e.g', <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('po', <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('lines', <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('purchase',\n",
       "  <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('order', <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('lines', <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = normalize(data)\n",
    "# s.get_tokens_data()\n",
    "# s.get_tokens_data_type()\n",
    "sbc = s.sort_by_token_type()\n",
    "# s.get_tokens_data_category()\n",
    "s.get_tokens_data_type(s.get_tokens_by_token_type(TokenTypes.CONTENT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T08:44:49.644308Z",
     "start_time": "2019-11-14T08:44:49.640614Z"
    }
   },
   "outputs": [],
   "source": [
    "def name_similarity_tokens(token_set1, token_set2):\n",
    "    sum1 = get_partial_similarity(token_set1, token_set2)\n",
    "    sum2 = get_partial_similarity(token_set2, token_set1)\n",
    "    \n",
    "    return (sum1 + sum2) / (len(token_set1) + len(token_set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T08:44:51.077265Z",
     "start_time": "2019-11-14T08:44:51.072085Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_partial_similarity(token_set1, token_set2, n=2):\n",
    "    total_sum = 0\n",
    "    for t1 in token_set1:\n",
    "        max_sim = -math.inf\n",
    "        for t2 in token_set2:\n",
    "            sim = compute_similarity_wordnet(t1.data, t2.data)\n",
    "            if math.isnan(sim):\n",
    "                sim = 1 - compute_similarity_ngram(t1.data, t2.data, n)\n",
    "                \n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                \n",
    "        total_sum = total_sum + max_sim\n",
    "    \n",
    "    return total_sum            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:18:26.311111Z",
     "start_time": "2019-11-14T14:18:26.306061Z"
    }
   },
   "outputs": [],
   "source": [
    "# the higher, the better\n",
    "def compute_similarity_wordnet(word1, word2):\n",
    "    allsyns1 = set(ss for ss in wn.synsets(word1))\n",
    "    allsyns2 = set(ss for ss in wn.synsets(word2))\n",
    "\n",
    "    if len(allsyns1) == 0 or len(allsyns2) == 0:\n",
    "        return math.nan\n",
    "    \n",
    "    best = max((wn.wup_similarity(s1, s2) or 0, s1, s2) for s1, s2 in product(allsyns1, allsyns2))\n",
    "#     print(best)\n",
    "    \n",
    "    return best[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T08:44:57.344492Z",
     "start_time": "2019-11-14T08:44:57.341404Z"
    }
   },
   "outputs": [],
   "source": [
    "# the lower, the better\n",
    "def compute_similarity_ngram(word1, word2, N):\n",
    "    ngram = NGram(N)\n",
    "    sim = ngram.distance(word1, word2)\n",
    "#     print(sim)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T18:39:09.082238Z",
     "start_time": "2019-11-12T18:39:09.024945Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6266666666666667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = \"PODeliverTo\"\n",
    "d2 = \"ShipTO\"\n",
    "\n",
    "s1 = normalize(d1)\n",
    "# s1.get_tokens_data_type()\n",
    "\n",
    "s2 = normalize(d2)\n",
    "# s2.get_tokens_data_type()\n",
    "\n",
    "name_similarity_tokens(s1.tokens, s2.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T08:44:58.771663Z",
     "start_time": "2019-11-14T08:44:58.765709Z"
    }
   },
   "outputs": [],
   "source": [
    "# max is 0.5\n",
    "def name_similarity_elements(element1, element2):\n",
    "    sum1 = 0 \n",
    "    sum2 = 0\n",
    "        \n",
    "    for tt in TokenTypes:\n",
    "        if tt == TokenTypes.SYMBOLS:\n",
    "            continue \n",
    "        t1 = element1.get_tokens_by_token_type(tt)\n",
    "        t2 = element2.get_tokens_by_token_type(tt)\n",
    "        \n",
    "        if len(t1) == 0 or len(t2) == 0:\n",
    "            continue\n",
    "            \n",
    "        sim = name_similarity_tokens(t1, t2)\n",
    "        sum1 = sum1 + tt.weight * sim\n",
    "        sum2 = sum2 + tt.weight * (len(t1) + len(t2))\n",
    "        \n",
    "    return sum1/sum2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T09:36:13.483548Z",
     "start_time": "2019-11-14T09:36:13.478846Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_lsim(element1, element2): \n",
    "    ns = name_similarity_elements(element1, element2)\n",
    "    max_c = -math.inf\n",
    "    for c1 in element1.categories:\n",
    "        c1 = normalize(c1)\n",
    "        for c2 in element2.categories:\n",
    "            c2 = normalize(c2)\n",
    "            nsc = name_similarity_elements(c1, c2)\n",
    "            if nsc > max_c:\n",
    "                max_c = nsc\n",
    "    \n",
    "    return ns * max_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:11:43.104186Z",
     "start_time": "2019-11-14T14:11:43.019552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('employee', <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>)]\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "d1 = \"employee\"\n",
    "d2 = \"employee-territory\"\n",
    "\n",
    "s1 = normalize(d1)\n",
    "s2 = normalize(d2)\n",
    "\n",
    "s1.add_category('CHAR(25)')\n",
    "s2.add_category('CHAR(25)')\n",
    "\n",
    "print(s1.get_tokens_data_type())\n",
    "\n",
    "lsim = compute_lsim(s1, s2)\n",
    "print(lsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T09:41:44.749407Z",
     "start_time": "2019-11-14T09:41:44.745850Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_weighted_similairty(ssim, lsim, w_struct=0.5):\n",
    "    return w_struct * ssim + (1 - w_struct) * lsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T09:41:45.678997Z",
     "start_time": "2019-11-14T09:41:45.673494Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_structural_matching(node_s, node_t, sims, th_accept=0.5):\n",
    "    s_leaves = list(map(lambda n: n.name.initial_name, node_s.leaves))\n",
    "    t_leaves = list(map(lambda n: n.name.initial_name, node_t.leaves))\n",
    "    all_leaves = product(s_leaves, t_leaves)\n",
    "    \n",
    "    filtered_pairs = [pair for pair in filter(lambda s: sims[s]['wsim'] > th_accept, sims.keys())\n",
    "                      if pair in all_leaves]\n",
    "    \n",
    "    return len(filtered_pairs) / (len(s_leaves) + len(t_leaves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T09:41:47.343150Z",
     "start_time": "2019-11-14T09:41:47.339458Z"
    }
   },
   "outputs": [],
   "source": [
    "def change_structural_similarity(leaves_s, leaves_t, sims, factor):\n",
    "    all_leaves = product(leaves_s, leaves_t)\n",
    "    for pair in all_leaves:\n",
    "        sims[pair]['ssim'] = sims[pair]['ssim'] * factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T09:43:31.801359Z",
     "start_time": "2019-11-14T09:43:31.787696Z"
    }
   },
   "outputs": [],
   "source": [
    "def tree_match(source_tree, target_tree, leaf_w_struct=0.5, w_struct=0.6, th_accept=0.14, th_high=0.15, \n",
    "               th_low=0.13, c_inc=1.2, c_dec=0.9):\n",
    "    \n",
    "    s_leaves = list(map(lambda n: n.name, source_tree.leaves))\n",
    "    t_leaves = list(map(lambda n: n.name, target_tree.leaves))\n",
    "    all_leaves = product(s_leaves, t_leaves)\n",
    "    sims = dict()\n",
    "    \n",
    "    for s, t in all_leaves:\n",
    "        ssim = name_similarity_elements(normalize(s.data_type), normalize(t.data_type))\n",
    "        lsim = compute_lsim(s, t)\n",
    "        wsim = compute_weighted_similairty(ssim, lsim, leaf_w_struct)\n",
    "        sims[(s.initial_name, t.initial_name)] = {'ssim': ssim, 'lsim': lsim, 'wsim': wsim}\n",
    "        \n",
    "    s_post_order = [node for node in PostOrderIter(source_tree)]\n",
    "    t_post_order = [node for node in PostOrderIter(target_tree)] \n",
    "    \n",
    "    for s in s_post_order:\n",
    "        if type(s.name) is not SchemaElement:\n",
    "            continue\n",
    "        \n",
    "        for t in t_post_order:\n",
    "            if type(t.name) is not SchemaElement:\n",
    "                continue\n",
    "                \n",
    "            if s.name not in s_leaves or t.name not in t_leaves:\n",
    "                ssim = compute_structural_matching(s, t, sims, th_accept)\n",
    "                lsim = compute_lsim(s.name, t.name)\n",
    "                wsim = compute_weighted_similairty(ssim, lsim, w_struct)\n",
    "                sims[(s.name.initial_name, t.name.initial_name)] = {'ssim': ssim, 'lsim': lsim, 'wsim': wsim}\n",
    "            \n",
    "            if sims[(s.name.initial_name, t.name.initial_name)]['wsim'] > th_high:\n",
    "                change_structural_similarity(list(map(lambda n: n.name.initial_name, s.leaves)), \n",
    "                                            list(map(lambda n: n.name.initial_name, t.leaves)), sims, c_inc)\n",
    "                \n",
    "            if sims[(s.name.initial_name, t.name.initial_name)]['wsim'] < th_low:\n",
    "                change_structural_similarity(list(map(lambda n: n.name.initial_name, s.leaves)), \n",
    "                                            list(map(lambda n: n.name.initial_name, t.leaves)), sims, c_dec)\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:09:34.358204Z",
     "start_time": "2019-11-14T14:09:34.343427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdb_schema\n",
      "├── <__main__.SchemaElement object at 0x12a7d2198>\n",
      "│   ├── <__main__.SchemaElement object at 0x10abc3358>\n",
      "│   ├── <__main__.SchemaElement object at 0x10abbe320>\n",
      "│   ├── <__main__.SchemaElement object at 0x12a7d2a58>\n",
      "│   ├── <__main__.SchemaElement object at 0x10abbe160>\n",
      "│   ├── <__main__.SchemaElement object at 0x10abbf0f0>\n",
      "│   ├── <__main__.SchemaElement object at 0x10abc32e8>\n",
      "│   └── <__main__.SchemaElement object at 0x10abc5240>\n",
      "└── <__main__.SchemaElement object at 0x12a7d9470>\n",
      "    ├── <__main__.SchemaElement object at 0x10abbe5c0>\n",
      "    └── <__main__.SchemaElement object at 0x10abc34e0>\n"
     ]
    }
   ],
   "source": [
    "employees = ['EmployeeID', 'FirstName', 'LastName', 'Title', 'EmailName', 'Extension', 'Workphone']\n",
    "et = ['EmployeeIdFk', 'TeritoryId']\n",
    "\n",
    "schema = Node('rdb_schema')\n",
    "\n",
    "emp = normalize('employee')\n",
    "emp.add_category('string')\n",
    "emp.data_type = 'string'\n",
    "employee = Node(emp, parent=schema)\n",
    "\n",
    "emp_ter = normalize('employee-teritory')\n",
    "emp_ter.add_category('string')\n",
    "emp_ter.data_type = 'string'\n",
    "teritory = Node(emp_ter, parent=schema)\n",
    "\n",
    "for e in employees:\n",
    "    sch = normalize(e)\n",
    "    sch.add_category('string')\n",
    "    sch.data_type = 'string'\n",
    "    n = Node(sch, parent=employee)\n",
    "    \n",
    "for e in et:\n",
    "    sch = normalize(e)\n",
    "    sch.add_category('str')\n",
    "    sch.data_type = 'str'\n",
    "    n = Node(sch, parent=teritory)\n",
    "    \n",
    "for pre, fill, node in RenderTree(schema):\n",
    "    print(\"%s%s\" % (pre, node.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T14:09:36.221090Z",
     "start_time": "2019-11-14T14:09:35.763147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('EmployeeID', 'employee-teritory'), {'ssim': 0.6666666666666666, 'lsim': 0.0718954248366013, 'wsim': 0.4287581699346405}), (('employee', 'employee-teritory'), {'ssim': 0.4444444444444444, 'lsim': 0.1323529411764706, 'wsim': 0.3196078431372549}), (('EmployeeID', 'EmployeeIdFk'), {'ssim': 0.324, 'lsim': 0.04125, 'wsim': 0.145625}), (('EmployeeID', 'TeritoryId'), {'ssim': 0.324, 'lsim': 0.03841145833333333, 'wsim': 0.14420572916666666}), (('Title', 'TeritoryId'), {'ssim': 0.243, 'lsim': 0.03510802469135802, 'wsim': 0.14255401234567902}), (('Extension', 'TeritoryId'), {'ssim': 0.243, 'lsim': 0.033950617283950615, 'wsim': 0.1419753086419753}), (('Extension', 'EmployeeIdFk'), {'ssim': 0.243, 'lsim': 0.024305555555555556, 'wsim': 0.1371527777777778}), (('FirstName', 'EmployeeIdFk'), {'ssim': 0.243, 'lsim': 0.023592105263157893, 'wsim': 0.13679605263157896}), (('LastName', 'EmployeeIdFk'), {'ssim': 0.243, 'lsim': 0.021076023391812866, 'wsim': 0.13553801169590643}), (('FirstName', 'TeritoryId'), {'ssim': 0.243, 'lsim': 0.020163143382352942, 'wsim': 0.13508157169117646}), (('EmailName', 'EmployeeIdFk'), {'ssim': 0.243, 'lsim': 0.019910990712074306, 'wsim': 0.13495549535603715}), (('LastName', 'TeritoryId'), {'ssim': 0.243, 'lsim': 0.0193359375, 'wsim': 0.13466796875}), (('EmailName', 'TeritoryId'), {'ssim': 0.243, 'lsim': 0.019186580882352942, 'wsim': 0.13459329044117646}), (('Title', 'EmployeeIdFk'), {'ssim': 0.243, 'lsim': 0.016493055555555552, 'wsim': 0.1332465277777778}), (('Workphone', 'TeritoryId'), {'ssim': 0.243, 'lsim': 0.015432098765432098, 'wsim': 0.13271604938271606}), (('Workphone', 'EmployeeIdFk'), {'ssim': 0.21870000000000003, 'lsim': 0.00954861111111111, 'wsim': 0.12977430555555555}), (('employee', 'EmployeeIdFk'), {'ssim': 0.125, 'lsim': 0.03841145833333333, 'wsim': 0.09036458333333333}), (('Extension', 'employee-teritory'), {'ssim': 0.0, 'lsim': 0.06617647058823528, 'wsim': 0.026470588235294114}), (('Title', 'employee-teritory'), {'ssim': 0.0, 'lsim': 0.051470588235294115, 'wsim': 0.020588235294117647}), (('EmailName', 'employee-teritory'), {'ssim': 0.0, 'lsim': 0.03758169934640523, 'wsim': 0.015032679738562092}), (('Workphone', 'employee-teritory'), {'ssim': 0.0, 'lsim': 0.033088235294117654, 'wsim': 0.013235294117647062}), (('employee', 'TeritoryId'), {'ssim': 0.0, 'lsim': 0.02199074074074074, 'wsim': 0.008796296296296297}), (('FirstName', 'employee-teritory'), {'ssim': 0.0, 'lsim': 0.021241830065359478, 'wsim': 0.008496732026143792}), (('LastName', 'employee-teritory'), {'ssim': 0.0, 'lsim': 0.021241830065359478, 'wsim': 0.008496732026143792})]\n",
      "[('EmployeeID', 'employee-teritory'), ('employee', 'employee-teritory'), ('EmployeeID', 'EmployeeIdFk'), ('EmployeeID', 'TeritoryId'), ('Title', 'TeritoryId'), ('Extension', 'TeritoryId')]\n"
     ]
    }
   ],
   "source": [
    "from operator import *\n",
    "\n",
    "sims = tree_match(employee, teritory)\n",
    "# print(sorted(sims, key=lambda x: sims[x]['wsim'], reverse=True))\n",
    "sorted_sims = sorted(sims.items(),key=lambda x:getitem(x[1],'wsim'), reverse=True)\n",
    "print(sorted_sims)\n",
    "# print(sims)\n",
    "# a = [node for node in PostOrderIter(employee)]\n",
    "# list(map(lambda n: n.name, a[2].leaves))[0].initial_name\n",
    "tuples = list(map(lambda x: x[0], filter(lambda s: s[1]['wsim'] > 0.14, sorted_sims)))\n",
    "\n",
    "print(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T12:22:00.695966Z",
     "start_time": "2019-11-14T12:22:00.241900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'etymology': '',\n",
       "  'definitions': [{'partOfSpeech': 'noun',\n",
       "    'text': ['NLP (uncountable)',\n",
       "     '(communication) Initialism of neuro-linguistic programming.',\n",
       "     '(computing) Initialism of natural language processing.',\n",
       "     '(mathematics) Initialism of nonlinear programming.',\n",
       "     '(psychology) Initialism of neuro-linguistic psychotherapy.'],\n",
       "    'relatedWords': [],\n",
       "    'examples': []},\n",
       "   {'partOfSpeech': 'proper noun',\n",
       "    'text': ['NLP', '(politics) Initialism of National Labour Party.'],\n",
       "    'relatedWords': [],\n",
       "    'examples': []}],\n",
       "  'pronunciations': {'text': [], 'audio': []}}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wiktionaryparser import WiktionaryParser\n",
    "\n",
    "parser = WiktionaryParser()\n",
    "word = parser.fetch('NLP')\n",
    "word"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
