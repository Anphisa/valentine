{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T19:47:16.299125Z",
     "start_time": "2019-11-13T19:47:16.294797Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import snakecase\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from similarity.ngram import NGram\n",
    "from itertools import product\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "from anytree import Node, RenderTree, PostOrderIter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T18:39:07.607603Z",
     "start_time": "2019-11-12T18:39:07.605851Z"
    }
   },
   "outputs": [],
   "source": [
    "data = 'POList. Order #. 01-Id.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T19:50:22.606488Z",
     "start_time": "2019-11-13T19:50:22.603791Z"
    }
   },
   "outputs": [],
   "source": [
    "data = 'Abbreviations and acronyms are expanded, e.g. {PO, Lines} 01 {Purchase, Order, Lines}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T18:39:07.641864Z",
     "start_time": "2019-11-12T18:39:07.639236Z"
    }
   },
   "outputs": [],
   "source": [
    "table = { 'name': 'nation',\n",
    "         'columns': [\n",
    "             { 'name': 'n_nationkey',\n",
    "             'type': 'INTEGER'\n",
    "             }, \n",
    "             { 'name': 'n_name',\n",
    "             'type': 'CHAR(25)'\n",
    "             },\n",
    "             { 'name': 'n_regionkey',\n",
    "             'type': 'INTEGER'\n",
    "             },\n",
    "             { 'name': 'n_comment',\n",
    "             'type': 'VARCHAR(152)'\n",
    "             }\n",
    "         ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T15:11:17.907091Z",
     "start_time": "2019-11-13T15:11:17.894546Z"
    }
   },
   "outputs": [],
   "source": [
    "class SchemaElement:\n",
    "    def __init__(self, name):\n",
    "        self.categories = list()\n",
    "        self.data_type = None # an element can belong to multiple categories\n",
    "        self.tokens = list()\n",
    "        self.initial_name = name\n",
    "#         self.table_name = None\n",
    "        \n",
    "    def add_category(self, category):\n",
    "        self.categories.append(category)\n",
    "        \n",
    "    def add_token(self, token):\n",
    "        if type(token) is Token:\n",
    "            self.tokens.append(token)\n",
    "        else:\n",
    "            print(\"Incorrect token type. The type should be 'Token'\")\n",
    "            \n",
    "    def get_tokens_data(self, tokens=None):\n",
    "        if tokens is None:\n",
    "            return list(map(lambda t: t.data, self.tokens)) \n",
    "        else:\n",
    "            return list(map(lambda t: t.data, tokens)) \n",
    "    \n",
    "    def get_tokens_data_type(self, tokens=None):\n",
    "        if tokens is None:\n",
    "            return list(map(lambda t: (t.data, t.token_type), self.tokens))\n",
    "        else:\n",
    "            return list(map(lambda t: (t.data, t.token_type), tokens))\n",
    "    \n",
    "    def sort_by_token_type(self):\n",
    "        return sorted(self.tokens, key=lambda token: token.token_type.token_name)\n",
    "    \n",
    "    def get_tokens_by_token_type(self, token_type):\n",
    "        sorted_tokens = self.sort_by_token_type()\n",
    "        return list(filter(lambda t: t.token_type == token_type, sorted_tokens))\n",
    "        \n",
    "class Token:\n",
    "    def __init__(self):\n",
    "        self.ignore = False\n",
    "        self.data = None\n",
    "        self.token_type = None\n",
    "        \n",
    "TokenType = namedtuple('TokenType', ['token_name', 'weight'])\n",
    "        \n",
    "class TokenTypes(Enum):\n",
    "    SYMBOLS = TokenType('symbols', 0)\n",
    "    NUMBER = TokenType('number', 0.1)\n",
    "    COMMON_WORDS = TokenType('common words', 0.1)\n",
    "    CONTENT = TokenType('content', 0.8)\n",
    "        \n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.value.weight\n",
    "    \n",
    "    @property\n",
    "    def token_name(self):\n",
    "        return self.value.token_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T13:00:43.729907Z",
     "start_time": "2019-11-13T13:00:43.724040Z"
    }
   },
   "outputs": [],
   "source": [
    "class Table:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.columns = list()\n",
    "        \n",
    "    def add_column(self, column_name, column_type=None):\n",
    "        schema_element = normalize(column_name)\n",
    "        if column_type:\n",
    "            schema_element.category = column_type\n",
    "        self.columns.append(schema_element)\n",
    "        \n",
    "    def get_all_columns(self):\n",
    "        return list(map(lambda c: c.get_tokens_data_type(), self.columns))\n",
    "    \n",
    "    def get_column_by_type(self, column_type):\n",
    "        return list(map(lambda c: c.get_tokens_data_type(),\n",
    "            filter(lambda c: column_type in c.category, self.columns)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T13:00:44.178935Z",
     "start_time": "2019-11-13T13:00:44.169923Z"
    }
   },
   "outputs": [],
   "source": [
    "class Schema:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.tables = list()\n",
    "        \n",
    "    def add_table_by_name(self, table_name):\n",
    "        table = Table(table_name)\n",
    "        self.tables.append(table)\n",
    "        \n",
    "    def add_table(self, table):\n",
    "        self.tables.append(table)\n",
    "                \n",
    "    def get_all_tables(self):\n",
    "        return list(map(lambda t: t.get_all_columns(), self.tables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T15:11:30.335416Z",
     "start_time": "2019-11-13T15:11:30.326005Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(element, schema_element=None):\n",
    "    if schema_element is None:\n",
    "        schema_element = SchemaElement(element)\n",
    "    tokens = nltk.word_tokenize(element)\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_obj = Token()\n",
    "        if token in string.punctuation:\n",
    "            token_obj.ignore = True\n",
    "            token_obj.data = token\n",
    "            token_obj.token_type = TokenTypes.SYMBOLS\n",
    "            token_obj.initial_name = token\n",
    "            schema_element.add_token(token_obj)\n",
    "        else:\n",
    "            try:\n",
    "                token_float = float(token)\n",
    "                token_obj.data = token\n",
    "                token_obj.token_type = TokenTypes.NUMBER\n",
    "                token_obj.initial_name = token\n",
    "                schema_element.add_token(token_obj)\n",
    "            except ValueError:\n",
    "                token_snake = snakecase.convert(token)\n",
    "                if '_' in token_snake:\n",
    "                    token_snake = token_snake.replace('_', ' ')\n",
    "                    schema_element = normalize(token_snake, schema_element)\n",
    "                elif token.lower() in stopwords.words('english'):\n",
    "                    token_obj.data = token.lower()\n",
    "                    token_obj.ignore = True\n",
    "                    token_obj.token_type = TokenTypes.COMMON_WORDS\n",
    "                    token_obj.initial_name = token\n",
    "                    schema_element.add_token(token_obj)\n",
    "                else:\n",
    "                    token_obj.data = token.lower()\n",
    "                    token_obj.token_type = TokenTypes.CONTENT\n",
    "                    token_obj.initial_name = token\n",
    "                    schema_element.add_token(token_obj)\n",
    "    \n",
    "    return schema_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T19:50:35.141875Z",
     "start_time": "2019-11-13T19:50:35.133116Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abbreviations',\n",
       "  <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('acronyms',\n",
       "  <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('expanded',\n",
       "  <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('e.g', <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('po', <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('lines', <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('purchase',\n",
       "  <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('order', <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>),\n",
       " ('lines', <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = normalize(data)\n",
    "# s.get_tokens_data()\n",
    "# s.get_tokens_data_type()\n",
    "sbc = s.sort_by_token_type()\n",
    "# s.get_tokens_data_category()\n",
    "s.get_tokens_data_type(s.get_tokens_by_token_type(TokenTypes.CONTENT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T13:01:00.251247Z",
     "start_time": "2019-11-13T13:01:00.247633Z"
    }
   },
   "outputs": [],
   "source": [
    "def name_similarity_tokens(token_set1, token_set2):\n",
    "    sum1 = get_partial_similarity(token_set1, token_set2)\n",
    "    sum2 = get_partial_similarity(token_set2, token_set1)\n",
    "    \n",
    "    return (sum1 + sum2) / (len(token_set1) + len(token_set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T13:01:00.563103Z",
     "start_time": "2019-11-13T13:01:00.558127Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_partial_similarity(token_set1, token_set2):\n",
    "    total_sum = 0\n",
    "    for t1 in token_set1:\n",
    "        max_sim = -math.inf\n",
    "        for t2 in token_set2:\n",
    "            sim = compute_similarity_wordnet(t1.data, t2.data)\n",
    "            if math.isnan(sim):\n",
    "                sim = 1 - compute_similarity_ngram(t1.data, t2.data, 2)\n",
    "                \n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                \n",
    "        total_sum = total_sum + max_sim\n",
    "    \n",
    "    return total_sum            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T13:01:01.083475Z",
     "start_time": "2019-11-13T13:01:01.078096Z"
    }
   },
   "outputs": [],
   "source": [
    "# the higher, the better\n",
    "def compute_similarity_wordnet(word1, word2):\n",
    "    allsyns1 = set(ss for ss in wn.synsets(word1))\n",
    "    if len(allsyns1) == 0:\n",
    "        return math.nan\n",
    "    allsyns2 = set(ss for ss in wn.synsets(word2))\n",
    "    \n",
    "    if len(allsyns2) == 0:\n",
    "        return math.nan\n",
    "    \n",
    "    best = max((wn.wup_similarity(s1, s2) or 0, s1, s2) for s1, s2 in product(allsyns1, allsyns2))\n",
    "#     print(best)\n",
    "    \n",
    "    return best[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T13:01:01.778612Z",
     "start_time": "2019-11-13T13:01:01.775248Z"
    }
   },
   "outputs": [],
   "source": [
    "# the lower, the better\n",
    "def compute_similarity_ngram(word1, word2, N):\n",
    "    ngram = NGram(N)\n",
    "    sim = ngram.distance(word1, word2)\n",
    "#     print(sim)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T18:39:09.082238Z",
     "start_time": "2019-11-12T18:39:09.024945Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6266666666666667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = \"PODeliverTo\"\n",
    "d2 = \"ShipTO\"\n",
    "\n",
    "s1 = normalize(d1)\n",
    "# s1.get_tokens_data_type()\n",
    "\n",
    "s2 = normalize(d2)\n",
    "# s2.get_tokens_data_type()\n",
    "\n",
    "name_similarity_tokens(s1.tokens, s2.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T15:56:44.611934Z",
     "start_time": "2019-11-13T15:56:44.606392Z"
    }
   },
   "outputs": [],
   "source": [
    "# max is 0.5\n",
    "def name_similarity_elements(element1, element2):\n",
    "    sum1 = 0 \n",
    "    sum2 = 0\n",
    "        \n",
    "    for tt in TokenTypes:\n",
    "        if tt == TokenTypes.SYMBOLS:\n",
    "            continue \n",
    "        t1 = element1.get_tokens_by_token_type(tt)\n",
    "        t2 = element2.get_tokens_by_token_type(tt)\n",
    "        \n",
    "        if len(t1) == 0 or len(t2) == 0:\n",
    "            continue\n",
    "            \n",
    "        sim = name_similarity_tokens(t1, t2)\n",
    "        sum1 = sum1 + tt.weight * sim\n",
    "        sum2 = sum2 + tt.weight * (len(t1) + len(t2))\n",
    "        \n",
    "    return sum1/sum2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T15:56:41.610032Z",
     "start_time": "2019-11-13T15:56:41.605539Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_lsim(element1, element2): \n",
    "    ns = name_similarity_elements(element1, element2)\n",
    "    max_c = -math.inf\n",
    "    for c1 in element1.categories:\n",
    "        c1 = normalize(c1)\n",
    "        for c2 in element2.categories:\n",
    "            c2 = normalize(c2)\n",
    "            nsc = name_similarity_elements(c1, c2)\n",
    "            if nsc > max_c:\n",
    "                max_c = nsc\n",
    "    \n",
    "    return ns * max_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T18:39:10.923115Z",
     "start_time": "2019-11-12T18:39:10.832999Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ship', <TokenTypes.CONTENT: TokenType(token_name='content', weight=0.8)>), ('to', <TokenTypes.COMMON_WORDS: TokenType(token_name='common words', weight=0.1)>)]\n",
      "0.5\n",
      "0.5\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "d1 = \"ShipTo\"\n",
    "d2 = \"ShipTO\"\n",
    "\n",
    "s1 = normalize(d1)\n",
    "s2 = normalize(d2)\n",
    "\n",
    "s1.add_category('CHAR(25)')\n",
    "s2.add_category('CHAR(25)')\n",
    "\n",
    "print(s1.get_tokens_data_type())\n",
    "\n",
    "lsim = compute_lsim(s1, s2)\n",
    "print(lsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T13:57:14.225052Z",
     "start_time": "2019-11-13T13:57:14.221514Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_weighted_similairty(ssim, lsim, w_struct=0.5):\n",
    "    return w_struct * ssim + (1 - w_struct) * lsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T15:52:02.316010Z",
     "start_time": "2019-11-13T15:52:02.310714Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_structural_matching(node_s, node_t, sims, th_accept=0.5):\n",
    "    s_leaves = list(map(lambda n: n.name, node_s.leaves))\n",
    "    t_leaves = list(map(lambda n: n.name, node_t.leaves))\n",
    "    all_leaves = product(s_leaves, t_leaves)\n",
    "    \n",
    "    filtered_pairs = [pair for pair in filter(lambda s: sims[s]['wsim'] > th_accept, sims.keys())\n",
    "                      if pair in all_leaves]\n",
    "    \n",
    "    return len(filtered_pairs) / (len(s_leaves) + len(t_leaves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T15:41:10.845872Z",
     "start_time": "2019-11-13T15:41:10.842250Z"
    }
   },
   "outputs": [],
   "source": [
    "def change_structural_similarity(leaves_s, leaves_t, sims, factor):\n",
    "    all_leaves = product(leaves_s, leaves_t)\n",
    "    for pair in all_leaves:\n",
    "        sims[pair]['ssim'] = sims[pair]['ssim'] * factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T19:49:27.657058Z",
     "start_time": "2019-11-13T19:49:27.643594Z"
    }
   },
   "outputs": [],
   "source": [
    "def tree_match(source_tree, target_tree, leaf_w_struct=0.5, w_struct=0.6, th_accept=0.12, th_high=0.13, \n",
    "               th_low=0.1, c_inc=1.2, c_dec=0.9):\n",
    "    \n",
    "    s_leaves = list(map(lambda n: n.name, source_tree.leaves))\n",
    "    t_leaves = list(map(lambda n: n.name, target_tree.leaves))\n",
    "    all_leaves = product(s_leaves, t_leaves)\n",
    "    sims = dict()\n",
    "    \n",
    "    for s, t in all_leaves:\n",
    "        ssim = name_similarity_elements(normalize(s.data_type), normalize(t.data_type))\n",
    "        lsim = compute_lsim(s, t)\n",
    "        wsim = compute_weighted_similairty(ssim, lsim)\n",
    "        sims[(s, t)] = {'ssim': ssim, 'lsim': lsim, 'wsim': wsim}\n",
    "        \n",
    "    s_post_order = [node for node in PostOrderIter(source_tree)]\n",
    "    t_post_order = [node for node in PostOrderIter(target_tree)] \n",
    "    \n",
    "    for s in s_post_order:\n",
    "        if type(s.name) is not SchemaElement:\n",
    "            continue\n",
    "        \n",
    "        for t in t_post_order:\n",
    "            if type(t.name) is not SchemaElement:\n",
    "                continue\n",
    "                \n",
    "            if s.name not in s_leaves or t.name not in t_leaves:\n",
    "                ssim = compute_structural_matching(s, t, sims)\n",
    "                lsim = compute_lsim(s.name, t.name)\n",
    "                wsim = compute_weighted_similairty(ssim, lsim, w_struct)\n",
    "                sims[(s, t)] = {'ssim': ssim, 'lsim': lsim, 'wsim': wsim}\n",
    "            \n",
    "            if sims[(s.name, t.name)]['wsim'] > th_high:\n",
    "                change_structural_similarity(list(map(lambda n: n.name, s.leaves)), \n",
    "                                            list(map(lambda n: n.name, t.leaves)), sims, c_inc)\n",
    "                \n",
    "            if sims[(s.name, t.name)]['wsim'] < th_low:\n",
    "                change_structural_similarity(list(map(lambda n: n.name, s.leaves)), \n",
    "                                            list(map(lambda n: n.name, t.leaves)), sims, c_dec)\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T19:47:47.318233Z",
     "start_time": "2019-11-13T19:47:47.304726Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdb_schema\n",
      "├── employee\n",
      "│   ├── <__main__.SchemaElement object at 0x12f7bf9b0>\n",
      "│   ├── <__main__.SchemaElement object at 0x12f623da0>\n",
      "│   ├── <__main__.SchemaElement object at 0x12f720550>\n",
      "│   ├── <__main__.SchemaElement object at 0x12f623ef0>\n",
      "│   ├── <__main__.SchemaElement object at 0x12f71a7b8>\n",
      "│   ├── <__main__.SchemaElement object at 0x12f720a20>\n",
      "│   └── <__main__.SchemaElement object at 0x12f7bfba8>\n",
      "└── employee-teritory\n",
      "    ├── <__main__.SchemaElement object at 0x12f623940>\n",
      "    └── <__main__.SchemaElement object at 0x12f7207f0>\n"
     ]
    }
   ],
   "source": [
    "employees = ['EmployeeID', 'FirstName', 'LastName', 'Title', 'EmailName', 'Extension', 'Workphone']\n",
    "et = ['EmployeeIdFk', 'TeritoryId']\n",
    "\n",
    "schema = Node('rdb_schema')\n",
    "employee = Node('employee', parent=schema)\n",
    "teritory = Node('employee-teritory', parent=schema)\n",
    "\n",
    "for e in employees:\n",
    "    sch = normalize(e)\n",
    "    sch.add_category('string')\n",
    "    sch.data_type = 'string'\n",
    "    n = Node(sch, parent=employee)\n",
    "    \n",
    "for e in et:\n",
    "    sch = normalize(e)\n",
    "    sch.add_category('str')\n",
    "    sch.data_type = 'str'\n",
    "    n = Node(sch, parent=teritory)\n",
    "    \n",
    "for pre, fill, node in RenderTree(schema):\n",
    "    print(\"%s%s\" % (pre, node.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T19:43:47.326381Z",
     "start_time": "2019-11-13T19:43:47.169188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmployeeID\n",
      "EmployeeIdFk\n",
      "\n",
      "EmployeeID\n",
      "TeritoryId\n",
      "\n",
      "Title\n",
      "TeritoryId\n",
      "\n",
      "Extension\n",
      "TeritoryId\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sims = tree_match(employee, teritory)\n",
    "# a = [node for node in PostOrderIter(employee)]\n",
    "# list(map(lambda n: n.name, a[2].leaves))[0].initial_name\n",
    "tuples = list(filter(lambda s: sims[s]['wsim'] > 0.14, sims.keys()))\n",
    "\n",
    "for s, t in tuples:\n",
    "    print(s.initial_name)\n",
    "    print(t.initial_name)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
